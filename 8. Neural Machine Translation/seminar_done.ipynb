{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgH3CAcubM4z"
   },
   "source": [
    "# Семинар  \n",
    "## Seq2Seq модели в машинном переводе\n",
    "\n",
    "На лекции мы подробно познакомились с подходами к решению задачи машинного перевода.\n",
    "Наиболее распространенными моделями последовательностей (seq2seq) являются модели кодер-декодер, которые (обычно) используют рекуррентную нейронную сеть (RNN) для кодирования исходного (входного) предложения в один вектор.  Вы можете думать о векторе контекста как об абстрактном представлении всего входного предложения. Этот вектор затем декодируется декодером, который учится выводить  предложение, генерируя его по одному слову за раз.\n",
    "\n",
    "\n",
    "$h_t = \\text{Encoder}(x_t, h_{t-1})$\n",
    "\n",
    "У нас есть последовательность $X = \\{x_1, x_2, ..., x_T\\}$, где $x_1 = \\text{<sos>;}, x_2 = \\text{the}$, и так далее. Начальное состояние, $h_0$,  может быть инициализировано вектором из нулей или обучаемым.\n",
    "\n",
    "\n",
    "Как только последнее слово, $x_T$, был подан на Encoder, мы  используем  информацию в  последнем скрытом состоянии, $h_T$, в зависимости от контекста вектор, т. е. $h_T $ это векторное представление всего исходного предложения.\n",
    "\n",
    "После получения вектора всего предложения мы можем декодировать предложение уже на новом языке. На каждом шаге декодирования мы подаем правильное слово $y_t$,  дополняем это информацией о скрытом состоянии $s_{t-1}$, где  $s_t = \\text{DecoderRNN}(y_t, s_{t-1})$\n",
    "\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/f6DQb.png)\n",
    "\n",
    "\n",
    "Мы всегда используем $<sos>$ для первого входа в декодер, $y_1$, но для последующих входов, $y_{\\text{from }t; 1}$, мы иногда будем использовать фактическое, основное истинное следующее слово в последовательности, $y_t$, а иногда использовать слово, предсказанное нашим декодером, $\\hat{y}_{t-1}$. Использование настоящих токенов в декодере называется Teacher Forcing [можно тут посмотреть](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет Multi30k\n",
    "\n",
    "Загрузим датасет и посмотрим на него. Не забудем, что надо токенизировать текст перед отправкой в модель. Мы будем  использовать TorchText и spaCy( как токенизатор) , чтобы помочь вам выполнить всю необходимую предварительную обработку быстрее чем мы делали раньше. В данной работе вам предлагается написать модель Seq2Seq и обучить ее на Multi30k. В данном задание мы будем подавать на вход перевернутые предложения, так как авторы seq2seq считали, что это улучшает качество перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fPuwHEnVIzn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQSnhb84VLU7"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L10vdpVaVXBo",
    "outputId": "9c0db0ce-c637-455b-b44c-afeb0a5fff5a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/adchumachenko/.local/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/adchumachenko/.local/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2018.1.18)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/adchumachenko/.local/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/adchumachenko/.local/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "/home/adchumachenko/.local/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from de_core_news_sm==2.3.0) (2.3.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.49.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/adchumachenko/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2018.1.18)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/adchumachenko/.local/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/adchumachenko/.local/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/adchumachenko/.local/lib/python3.6/site-packages/de_core_news_sm -->\n",
      "/home/adchumachenko/.local/lib/python3.6/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "! python3 -m spacy download en\n",
    "! python3 -m spacy download de\n",
    "\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ferOqkOUVirW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adchumachenko/.local/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)][::-1]\n",
    "\n",
    "# немецкий язык является полем SRC, а английский в поле TRG\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6pNY6cWW3j5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adchumachenko/.local/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# В датасете содержится ~ 30к предложений средняя длина которых 11\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOS3e7QZbLro"
   },
   "source": [
    "Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "r0Xpf4IBW4Uf",
    "outputId": "670cffee-7a16-4f45-8140-501775f67f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train : 29000\n",
      "Number of sentences in validation : 1014\n",
      "Number of sentences in test : 1000\n"
     ]
    }
   ],
   "source": [
    "labels = ['train', 'validation', 'test']\n",
    "dataloaders = [train_data, valid_data, test_data]\n",
    "for d, l in zip(dataloaders, labels):\n",
    "    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Gg63m8haW4XC",
    "outputId": "16d7b80b-2308-4bb8-8951-35a280d4d0f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in source vocabulary 7854\n",
      "Number of words in source vocabulary 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(\"Number of words in source vocabulary\", len(SRC.vocab))\n",
    "print(\"Number of words in source vocabulary\", len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "-FhP6bJOzASS",
    "outputId": "bc965b74-f90c-4ab1-f3e1-1ac42ee92c35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2],\n",
       "        [5885],\n",
       "        [3832],\n",
       "        [   0],\n",
       "        [ 664],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [5885],\n",
       "        [3832],\n",
       "        [   0],\n",
       "        [ 664],\n",
       "        [   0],\n",
       "        [5885],\n",
       "        [5015],\n",
       "        [   0],\n",
       "        [5885],\n",
       "        [   0],\n",
       "        [5015],\n",
       "        [   0],\n",
       "        [5885],\n",
       "        [4970],\n",
       "        [   4],\n",
       "        [   3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.process([\"ein klein gespenster.\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ein'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.itos[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "Чтобы сравнивать моделей, надо иметь численную метрику качества. Качество перевода можно оценить с помощью метрики **BLEU**. Как она работает:\n",
    "\n",
    "Пусть у нас есть:\n",
    "\n",
    "Предложенный перевод: `the cat mat`\n",
    "\n",
    "Эталонный перевод: `the cat is on the mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [\"the\", \"cat\", \"mat\"]\n",
    "target = [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем н-граммы из предложенного переводы и посчитаем, сколько из них содержится в эталонном переводе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams in pred\n",
    "unigrams = [(i,) for i in pred]\n",
    "bigrams = [(i, j) for i, j in zip(pred[:-1], pred[1:])]\n",
    "trigrams = [tuple(pred)]\n",
    "\n",
    "# n-grams in target\n",
    "target_unigrams = [(i,) for i in target]\n",
    "target_bigrams = [(i, j) for i, j in zip(target[:-1], target[1:])]\n",
    "target_trigrams = [(i, j, k) \n",
    "                   for i, j, k in zip(target[:-2], target[1:-1], target[2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unigrams = sum(uni in target_unigrams for uni in unigrams)\n",
    "count_bigrams = sum(bi in target_bigrams for bi in bigrams)\n",
    "count_trigrams = sum(tri in target_trigrams for tri in trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для BLEU нам надо поделить число посчитанных н-грамм на их количество. Если мы смотрели на разные н-граммы, то BLEU будет являться взвешенной суммой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-uni: 1.0\n",
      "BLEU-bi: 0.5\n",
      "BLEU-tri: 0.0\n"
     ]
    }
   ],
   "source": [
    "bleu_uni = count_unigrams / len(unigrams)\n",
    "bleu_bi = count_bigrams / len(bigrams)\n",
    "bleu_tri = count_trigrams / len(trigrams)\n",
    "print(f\"BLEU-uni: {bleu_uni}\\nBLEU-bi: {bleu_bi}\\nBLEU-tri: {bleu_tri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.5\n"
     ]
    }
   ],
   "source": [
    "bleu = (bleu_uni + bleu_bi + bleu_tri) / 3\n",
    "assert bleu == 0.5\n",
    "print(f\"BLEU: {bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считать вручную эту метрику не требуется. Уже есть готовые реализации, например в библиотеке `nltk`. Эта реализация использует модифицированные подсчет н-грамм и усреднение bleu для каждой н-граммы, поэтому это число будет отличаться от простой реализации. \n",
    "\n",
    "Заметим, что для посчета могут использоваться несколько эталонных переводов. Поэтому эталонные переводы передаются списком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.2601300475114445\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "print(f\"BLEU: {sentence_bleu([target], pred, weights=(1/2, 1/2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU – не единственная метрика для перевода. Ещё есть ROGUE, METEOR, WER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSd3la5FbJ5_"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Напишем для начала простой Encoder, который реализует следующий функционал:\n",
    "\n",
    "$ (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n",
    "\n",
    "В  методе forward мы передаем исходное предложение $X$, которое преобразуется в embeddings, к которым применяется dropout . Эти вектора затем передаются в RNN. Когда мы передадим всю последовательность RNN, он автоматически выполнит для нас рекуррентный расчет скрытых состояний по всей последовательности! Вы можете заметить, что мы не передаем начальное скрытое или состояние ячейки в RNN. Это происходит потому, что, как отмечено в документации, если никакое скрытое состояние/ячейка не передается RNN, он автоматически создаст начальное скрытое состояние/ячейка как тензор всех нулей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ar5SN6tW4ck"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    " \n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "        ) \n",
    "        #(lstm embd, hid, layers, dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param: src sentences (src_len x batch_size)\n",
    "        \"\"\"\n",
    "        # embedded = <TODO> (src_len x batch_size x embd_dim)\n",
    "        embedded = self.embedding(src)\n",
    "        # dropout over embedding\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # [Attention return is for lstm, but you can also use gru]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8QOCpKxfD3M"
   },
   "source": [
    "## Decoder\n",
    "Похожий на Encoder, но со слоем проекцией, который переводит из hidden_dim в output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRgtzaf4bJp6"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size, num_layer=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, emb_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            emb_size, hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        #(lstm embd, hid, layers, dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # Projection :hid_dim x output_dim\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, cell):\n",
    "        \n",
    "        \n",
    "        # (1x batch_size)\n",
    "        input_ = input_.unsqueeze(0)\n",
    "        \n",
    "        # (1 x batch_size x emb_dim)\n",
    "        embedded = self.embedding(input_) # embd over input and dropout \n",
    "        embedded = self.dropout(embedded)\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #sent len and n directions will always be 1 in the decoder\n",
    "        \n",
    "        # (batch_size x output_dim)\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0)) #project out of the rnn on the output dim \n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlD7-nusfL86"
   },
   "source": [
    "## Seq2Seq module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_YvVGzaW4fY"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        # Hidden dimensions of encoder and decoder must be equal\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self._init_weights()  \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        :param: src (src_len x batch_size)\n",
    "        :param: tgt\n",
    "        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src) # TODO pass src throw encoder\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input_ = trg[0] # TODO trg[idxs]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input_, hidden, cell) #TODO pass state and input throw decoder \n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input_ = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        p = 0.08\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param.data, -p, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msbn2VypfUur"
   },
   "outputs": [],
   "source": [
    "input_size = len(SRC.vocab)\n",
    "output_size = len(TRG.vocab)\n",
    "src_emb_size =  tgt_emb_size = 128\n",
    "hidden_size = 256\n",
    "num_layers =  3\n",
    "dropout = 0.2\n",
    "\n",
    "batch_size = 128\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                  batch_size = batch_size, device = device)\n",
    "train_iterator, valid_iterator, test_iterator = iterators\n",
    "\n",
    "encoder = Encoder(input_size, src_emb_size, hidden_size, num_layers, dropout)\n",
    "decoder = Decoder(output_size, tgt_emb_size, hidden_size, num_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vaUeDjTfU4k"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def training(model, iterator, optimizer, criterion, clip): \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfbTx2FMjaIM"
   },
   "outputs": [],
   "source": [
    "def validating(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing !!\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "lQV_yqkLjcyQ",
    "outputId": "fd15db8a-bb0c-46c0-fdb6-0feb837a28b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7c7d68799d4507ae4987a4e2470fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 \n",
      " Train Loss 2.91569  Val loss 4.12263:\n",
      "Train Perplexity 18.461546476054888  Val Perplexity 61.7213561579101:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea23a01886a411499545ef61ab4141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \n",
      " Train Loss 2.47883  Val loss 4.17133:\n",
      "Train Perplexity 11.927301308492034  Val Perplexity 64.80158092167129:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e85c04bb31e44c5995cd521e9584859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2 \n",
      " Train Loss 2.34536  Val loss 4.0779:\n",
      "Train Perplexity 10.43702938188804  Val Perplexity 59.021394687715386:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7696901f7f9b407f8675b66d54aa1807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3 \n",
      " Train Loss 2.21608  Val loss 4.04541:\n",
      "Train Perplexity 9.171308778307111  Val Perplexity 57.13460642127021:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380f80620b334d2dba4dda7d9ef41bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4 \n",
      " Train Loss 2.15665  Val loss 4.15775:\n",
      "Train Perplexity 8.642137948941816  Val Perplexity 63.92752373356097:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabdc8b98b834253af16ae55145ada9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5 \n",
      " Train Loss 2.07391  Val loss 4.02728:\n",
      "Train Perplexity 7.955869833014664  Val Perplexity 56.10808950033099:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec44edc3f77e4c00b93ff4ca71116e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6 \n",
      " Train Loss 2.0324  Val loss 4.07283:\n",
      "Train Perplexity 7.632382112163145  Val Perplexity 58.722913505813594:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362310cc966b48e19bd6aef8dd6371c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7 \n",
      " Train Loss 1.97908  Val loss 3.84578:\n",
      "Train Perplexity 7.236082725803739  Val Perplexity 46.79517035239663:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c237edff864624b1fdb2553b729d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8 \n",
      " Train Loss 1.91944  Val loss 3.80296:\n",
      "Train Perplexity 6.817139801874618  Val Perplexity 44.833696019709016:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9794473f16406b85bedfa9cc9c36b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9 \n",
      " Train Loss 1.84702  Val loss 3.72253:\n",
      "Train Perplexity 6.340895471265397  Val Perplexity 41.368925201780684:\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "CLIP = 1\n",
    "\n",
    "# TODO\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    train_loss = round(training(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
    "    valid_loss = round(validating(model, valid_iterator, criterion),5)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n",
    "    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "n5Zf6Kb1jhOI",
    "outputId": "55d423b7-19f7-48d0-cf66-c698eaecef53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.872852861881256 Test PPL:48.07935436947382|\n"
     ]
    }
   ],
   "source": [
    "test_loss = validating(model, test_iterator, criterion)\n",
    "\n",
    "print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjvFmEYR_lkn"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sent_vec = SRC.process([sentence]).to(device)\n",
    "    input = torch.zeros((10, 1)).type(torch.LongTensor).to(device)\n",
    "    input += SRC.vocab.stoi['<sos>']\n",
    "    output = model(input, input, teacher_forcing_ratio = 0) # get model output\n",
    "    out = []\n",
    "    for t in output:\n",
    "        if t[0].max(0)[1] != SRC.vocab.stoi['<eos>']:\n",
    "            out += [TRG.vocab.itos[t[0].max(0)[1]]]\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(out[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ETSmEb_d_q9-",
    "outputId": "f91a1b1b-3bc5-4269-fec1-145c110f5870"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a group of is playing in the . gyro'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"ein klein gespenster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYpSVKDgGBP5"
   },
   "outputs": [],
   "source": [
    "def bleu(sentence, target):\n",
    "    hyp = translate(sentence)\n",
    "    return sentence_bleu([target], hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.08714939065202e-155"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(\"ein klein gespenster\", \"a little ghost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]simple_seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
