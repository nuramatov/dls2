{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgH3CAcubM4z"
   },
   "source": [
    "# Семинар  \n",
    "## Seq2Seq модели в машинном переводе\n",
    "\n",
    "На лекции мы подробно познакомились с подходами к решению задачи машинного перевода.\n",
    "Наиболее распространенными моделями последовательностей (seq2seq) являются модели кодер-декодер, которые (обычно) используют рекуррентную нейронную сеть (RNN) для кодирования исходного (входного) предложения в один вектор.  Вы можете думать о векторе контекста как об абстрактном представлении всего входного предложения. Этот вектор затем декодируется декодером, который учится выводить  предложение, генерируя его по одному слову за раз.\n",
    "\n",
    "\n",
    "$h_t = \\text{Encoder}(x_t, h_{t-1})$\n",
    "\n",
    "У нас есть последовательность $X = \\{x_1, x_2, ..., x_T\\}$, где $x_1 = \\text{<sos>;}, x_2 = \\text{the}$, и так далее. Начальное состояние, $h_0$,  может быть инициализировано вектором из нулей или обучаемым.\n",
    "\n",
    "\n",
    "Как только последнее слово, $x_T$, был подан на Encoder, мы  используем  информацию в  последнем скрытом состоянии, $h_T$, в зависимости от контекста вектор, т. е. $h_T $ это векторное представление всего исходного предложения.\n",
    "\n",
    "После получения вектора всего предложения мы можем декодировать предложение уже на новом языке. На каждом шаге декодирования мы подаем правильное слово $y_t$,  дополняем это информацией о скрытом состоянии $s_{t-1}$, где  $s_t = \\text{DecoderRNN}(y_t, s_{t-1})$\n",
    "\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/f6DQb.png)\n",
    "\n",
    "\n",
    "Мы всегда используем $<sos>$ для первого входа в декодер, $y_1$, но для последующих входов, $y_{\\text{from }t; 1}$, мы иногда будем использовать фактическое, основное истинное следующее слово в последовательности, $y_t$, а иногда использовать слово, предсказанное нашим декодером, $\\hat{y}_{t-1}$. Использование настоящих токенов в декодере называется Teacher Forcing [можно тут посмотреть](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет Multi30k\n",
    "\n",
    "Загрузим датасет и посмотрим на него. Не забудем, что надо токенизировать текст перед отправкой в модель. Мы будем  использовать TorchText и spaCy( как токенизатор) , чтобы помочь вам выполнить всю необходимую предварительную обработку быстрее чем мы делали раньше. В данной работе вам предлагается написать модель Seq2Seq и обучить ее на Multi30k. В данном задание мы будем подавать на вход перевернутые предложения, так как авторы seq2seq считали, что это улучшает качество перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fPuwHEnVIzn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQSnhb84VLU7"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L10vdpVaVXBo",
    "outputId": "9c0db0ce-c637-455b-b44c-afeb0a5fff5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python3 -m spacy download en\n",
    "! python3 -m spacy download de\n",
    "\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ferOqkOUVirW"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# немецкий язык является полем SRC, а английский в поле TRG\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6pNY6cWW3j5"
   },
   "outputs": [],
   "source": [
    "# В датасете содержится ~ 30к предложений средняя длина которых 11\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOS3e7QZbLro"
   },
   "source": [
    "Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "r0Xpf4IBW4Uf",
    "outputId": "670cffee-7a16-4f45-8140-501775f67f1a"
   },
   "outputs": [],
   "source": [
    "labels = ['train', 'validation', 'test']\n",
    "dataloaders = [train_data, valid_data, test_data]\n",
    "for d, l in zip(dataloaders, labels):\n",
    "    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Gg63m8haW4XC",
    "outputId": "16d7b80b-2308-4bb8-8951-35a280d4d0f6"
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(\"Number of words in source vocabulary\", len(SRC.vocab))\n",
    "print(\"Number of words in source vocabulary\", len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "-FhP6bJOzASS",
    "outputId": "bc965b74-f90c-4ab1-f3e1-1ac42ee92c35"
   },
   "outputs": [],
   "source": [
    "SRC.process([\"ein klein gespenster\", ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "Чтобы сравнивать моделей, надо иметь численную метрику качества. Качество перевода можно оценить с помощью метрики **BLEU**. Как она работает:\n",
    "\n",
    "Пусть у нас есть:\n",
    "\n",
    "Предложенный перевод: `the cat mat`\n",
    "\n",
    "Эталонный перевод: `the cat is on the mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [\"the\", \"cat\", \"mat\"]\n",
    "target = [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем н-граммы из предложенного переводы и посчитаем, сколько из них содержится в эталонном переводе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams in pred\n",
    "unigrams = [(i,) for i in pred]\n",
    "bigrams = [(i, j) for i, j in zip(pred[:-1], pred[1:])]\n",
    "trigrams = [tuple(pred)]\n",
    "\n",
    "# n-grams in target\n",
    "target_unigrams = ...\n",
    "target_bigrams = ...\n",
    "target_trigrams = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unigrams = sum(uni in target_unigrams for uni in unigrams)\n",
    "count_bigrams = sum(bi in target_bigrams for bi in bigrams)\n",
    "count_trigrams = sum(tri in target_trigrams for tri in trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для BLEU нам надо поделить число посчитанных н-грамм на их количество. Если мы смотрели на разные н-граммы, то BLEU будет являться взвешенной суммой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_uni = ...\n",
    "bleu_bi = ...\n",
    "bleu_tri = ...\n",
    "print(f\"BLEU-uni: {bleu_uni}\\nBLEU-bi: {bleu_bi}\\nBLEU-tri: {bleu_tri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = (bleu_uni + bleu_bi + bleu_tri) / 3\n",
    "assert bleu == 0.5\n",
    "print(f\"BLEU: {bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считать вручную эту метрику не требуется. Уже есть готовые реализации, например в библиотеке `nltk`. Эта реализация использует модифицированные подсчет н-грамм и усреднение bleu для каждой н-граммы, поэтому это число будет отличаться от простой реализации. \n",
    "\n",
    "Заметим, что для посчета могут использоваться несколько эталонных переводов. Поэтому эталонные переводы передаются списком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "print(f\"BLEU: {sentence_bleu([target], pred, weights=(1/2, 1/2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU – не единственная метрика для перевода. Ещё есть ROGUE, METEOR, WER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSd3la5FbJ5_"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Напишем для начала простой Encoder, который реализует следующий функционал:\n",
    "\n",
    "$ (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n",
    "\n",
    "В  методе forward мы передаем исходное предложение $X$, которое преобразуется в embeddings, к которым применяется dropout . Эти вектора затем передаются в RNN. Когда мы передадим всю последовательность RNN, он автоматически выполнит для нас рекуррентный расчет скрытых состояний по всей последовательности! Вы можете заметить, что мы не передаем начальное скрытое или состояние ячейки в RNN. Это происходит потому, что, как отмечено в документации, если никакое скрытое состояние/ячейка не передается RNN, он автоматически создаст начальное скрытое состояние/ячейка как тензор всех нулей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ar5SN6tW4ck"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = ...\n",
    " \n",
    "        self.rnn = ... #(lstm embd, hid, layers, dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param: src sentences (src_len x batch_size)\n",
    "        \"\"\"\n",
    "        # embedded = <TODO> (src_len x batch_size x embd_dim)\n",
    "        embedded = ...\n",
    "        # dropout over embedding\n",
    "        embedded = ...\n",
    "        outputs, (hidden, cell) = ...\n",
    "        # [Attention return is for lstm, but you can also use gru]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8QOCpKxfD3M"
   },
   "source": [
    "## Decoder\n",
    "Похожий на Encoder, но со слоем проекцией, который переводит из hidden_dim в output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRgtzaf4bJp6"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size, num_layer=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = ...\n",
    "        \n",
    "        self.rnn = ... #(lstm embd, hid, layers, dropout)\n",
    "        \n",
    "        self.out = ... # Projection :hid_dim x output_dim\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, cell):\n",
    "        \n",
    "        \n",
    "        # (1x batch_size)\n",
    "        input_ = input_.unsqueeze(0)\n",
    "        \n",
    "        # (1 x batch_size x emb_dim)\n",
    "        embedded = ... # embd over input and dropout \n",
    "        embedded = ...\n",
    "                \n",
    "        output, (hidden, cell) = ...\n",
    "        \n",
    "        #sent len and n directions will always be 1 in the decoder\n",
    "        \n",
    "        # (batch_size x output_dim)\n",
    "        \n",
    "        prediction = ... #project out of the rnn on the output dim \n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlD7-nusfL86"
   },
   "source": [
    "## Seq2Seq module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_YvVGzaW4fY"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        # Hidden dimensions of encoder and decoder must be equal\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self._init_weights()  \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        :param: src (src_len x batch_size)\n",
    "        :param: tgt\n",
    "        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = ... # TODO pass src throw encoder\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = ... # TODO trg[idxs]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            output, hidden, cell = ... #TODO pass state and input throw decoder \n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        p = 0.08\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param.data, -p, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msbn2VypfUur"
   },
   "outputs": [],
   "source": [
    "input_size = len(SRC.vocab)\n",
    "output_size = len(TRG.vocab)\n",
    "src_emb_size =  tgt_emb_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers =  3\n",
    "dropout = 0.2\n",
    "\n",
    "batch_size = 128\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                  batch_size = batch_size, device = device)\n",
    "train_iterator, valid_iterator, test_iterator = iterators\n",
    "\n",
    "encoder = Encoder(input_size, src_emb_size, hidden_size, num_layers, dropout)\n",
    "decoder = Decoder(output_size, tgt_emb_size, hidden_size, num_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vaUeDjTfU4k"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def training(model, iterator, optimizer, criterion, clip): \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfbTx2FMjaIM"
   },
   "outputs": [],
   "source": [
    "def validating(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing !!\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "lQV_yqkLjcyQ",
    "outputId": "fd15db8a-bb0c-46c0-fdb6-0feb837a28b4"
   },
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "CLIP = 1\n",
    "\n",
    "# TODO\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    train_loss = round(training(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
    "    valid_loss = round(validating(model, valid_iterator, criterion),5)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n",
    "    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "n5Zf6Kb1jhOI",
    "outputId": "55d423b7-19f7-48d0-cf66-c698eaecef53"
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjvFmEYR_lkn"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sent_vec = SRC.process([sentence]).to(device)\n",
    "    input = torch.zeros((10, 1)).type(torch.LongTensor).to(device)\n",
    "    input += SRC.vocab.stoi['<sos>']\n",
    "    output = ... # get model output\n",
    "    for t in output:\n",
    "        if t[0].max(0)[1] != SRC.vocab.stoi['<eos>']:\n",
    "            # print next token\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ETSmEb_d_q9-",
    "outputId": "f91a1b1b-3bc5-4269-fec1-145c110f5870"
   },
   "outputs": [],
   "source": [
    "translate(\"ein klein gespenster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYpSVKDgGBP5"
   },
   "outputs": [],
   "source": [
    "def bleu(sentence, target):\n",
    "    sent_vec = SRC.process([sentence]).to(device)\n",
    "    input = torch.zeros((10, 1)).type(torch.LongTensor).to(device)\n",
    "    input += SRC.vocab.stoi['<sos>']\n",
    "    output = model(sent_vec, input, 0)\n",
    "    hyp = \"\"\n",
    "    for t in output:\n",
    "        if t[0].max(0)[1] != SRC.vocab.stoi['<eos>']:\n",
    "            hyp += TRG.vocab.itos[t[0].max(0)[1]] + \" \"\n",
    "    return sentence_bleu([target], hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu(\"ein klein gespenster\", \"a little ghost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]simple_seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
