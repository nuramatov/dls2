{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_RNN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUI02UFXY5lZ"
      },
      "source": [
        "# Архитектура RNN. Классификация текста.\n",
        "\n",
        "Привет! В это семинаре мы познакомимся с задачей классификации текста, на примере поиска тематики новости, а также с двумя из основных архитектурт рекуррентных нейросетей – RNN и GRU.\n",
        "\n",
        "Нам потребуется одна библиотека от `HuggingFace🤗` под названием `datasets`. Она содержит большое число датасетов, которые используются в NLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eSlBJXbpNhH"
      },
      "source": [
        "# !pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwNeXLCXr4ue"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBGF3mNQKAgN"
      },
      "source": [
        "# За детерминизм!\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed_all(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-nNNuqZ9GN"
      },
      "source": [
        "Загрузим датасет новостей: `AgNews`. В нем разделены тексты на 4 темы: `World`, `Sports`, `Business`, `Sci/Tech`. Посмотрим на структуру датасета и на примеры текстов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LREB3dpscnH"
      },
      "source": [
        "dataset = datasets.load_dataset(\"ag_news\")\n",
        "dataset[\"train\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YysP5HpsiBX"
      },
      "source": [
        "dataset[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QK1tDYKaTJa"
      },
      "source": [
        "В `dataset` находятся `train` и `test` части датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjEB-Yv09AQo"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSIXBJPaegW"
      },
      "source": [
        "Чтобы превращать текст из набора слов в набор векторов мы будем использовать предобученные эмбеддинги. Посмотрим на их список и выберем один из них."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGW4jSiswVDy"
      },
      "source": [
        "print(\"\\n\".join(api.info()['models'].keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1RzPKhwC3q"
      },
      "source": [
        "word2vec = api.load(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAIznT0Kaue8"
      },
      "source": [
        "Объеденим `dataset`, `word2vec` в один объект, который будет возвращать массив из векторов-эмбеддингов и класс текста."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdVbtWJzszbu"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class AgNewsDataset(Dataset):\n",
        "    def __init__(self, word2vec, train=True, max_length=128):\n",
        "        self.data = dataset[\"train\"] if train else dataset[\"test\"]\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        self.word2vec = word2vec\n",
        "        self.max_length = max_length # Лучше ограничить длину текстов\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[item][\"text\"]\n",
        "        tokens = ... # Получи токены\n",
        "        embeds = ... # Собери эмбеддинги\n",
        "        return {\"inputs\": (np.array(embeds) - self.mean) / self.std, \"label\": self.data[item][\"label\"]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-8eQDPJbIJl"
      },
      "source": [
        "Обычно для сбора объектов в батчи просто собирают фиксированное число объектов. В случае текстов это не работает – каждый текст имеет произвольный размер. Можно дополнить предложения нулями, но это не эффективно. А можно собирать тексты в батчи на основе их длин. Тогда будем ограничивать число токенов в одном батче, и пока это число не было превышено, добавлять новые тексты в батч.\n",
        "\n",
        "Для более эффективного распределения ресурсов надо рассортировать тексты по длинам. Но простая реализация этого не позволяет обучать модель эффективно(батчи не будут случайны). Поэтому на семинаре рассмотрим более простую реализацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwDQaeh7s4tF"
      },
      "source": [
        "from torch.utils.data import Sampler, RandomSampler\n",
        "\n",
        "\n",
        "class TextSampler(Sampler):\n",
        "    def __init__(self, sampler, batch_size_tokens=1e5):\n",
        "        self.sampler = sampler\n",
        "        self.batch_size_tokens = batch_size_tokens\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch = []\n",
        "        max_len = 0\n",
        "        for ix in self.sampler:\n",
        "            row = self.sampler.data_source[ix]\n",
        "            max_len = ... # Обнови длину текстов\n",
        "            if (len(batch) + 1) * max_len > self.batch_size_tokens:\n",
        "                ... # Если batch окажется слишком большим, то верни его и обнови batch\n",
        "            batch.append(ix)\n",
        "        if len(batch) > 0:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sampler)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzwuahwBxb2l"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    max_len = max(len(row[\"inputs\"]) for row in batch)\n",
        "    input_embeds = np.zeros((len(batch), max_len, word2vec.vector_size))\n",
        "    labels = np.zeros((len(batch),))\n",
        "    for idx, row in enumerate(batch):\n",
        "        input_embeds[idx][:len(row[\"inputs\"])] += row[\"inputs\"]\n",
        "        labels[idx] = row[\"label\"]\n",
        "    return {\"inputs\": torch.FloatTensor(input_embeds), \"labels\": torch.LongTensor(labels)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YziKZLiUwva0"
      },
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "\n",
        "\n",
        "train_dataset = AgNewsDataset(word2vec, train=True)\n",
        "valid_dataset = AgNewsDataset(word2vec, train=False)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_sampler=TextSampler(train_sampler), collate_fn=collate_fn, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_sampler=TextSampler(valid_sampler), collate_fn=collate_fn, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsqpZ-WfPqe"
      },
      "source": [
        "## CNN\n",
        "\n",
        "Первая модель, которую мы рассмотрим: CNN. Одномерная конволюция достаточно хорошо справляется с задачей классификации. В конце надо собрать вектор текста с помощью `AdaptiveMaxPool1d` или `AdiptiveAvgPool1d`. Для классиффикации можно собрать любую Feed Forward Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9DMopWCiBlD"
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = ...\n",
        "        self.cl = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        features = self.cnn(x)\n",
        "        prediction = self.cl(features)\n",
        "        return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ed-3nQHjDrd"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CNNModel(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oZjYEBkgauj"
      },
      "source": [
        "Подготовим функцию для обучения модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Smd5_03CSp"
      },
      "source": [
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def training(model, criterion, optimizer, num_epochs, max_grad_norm=0):\n",
        "    for e in range(num_epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader)\n",
        "        for batch in pbar:\n",
        "            input_embeds = batch[\"inputs\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            ... # Обнови параметры модели. \n",
        "            # Посмотри, как работает clip_grad_norm, и добавь его в процесс получения градиентов\n",
        "            # if max_grad_norm is not None:\n",
        "            #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            pbar.update(labels.size(0))\n",
        "        valid_loss = 0\n",
        "        valid_acc = 0\n",
        "        num_iter = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_embeds = batch[\"inputs\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # Получи предсказания модели и посчитай метрики (лосс и точность)\n",
        "                num_iter += 1\n",
        "\n",
        "        print(f\"Valid Loss: {valid_loss / num_iter}, accuracy: {valid_acc / num_iter}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOi77V2XjDpg"
      },
      "source": [
        "training(model, criterion, optimizer, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN8E8-ZSgnul"
      },
      "source": [
        "## RNN\n",
        "\n",
        "Вторая модель: RNN. Это рекуррентная сеть, она использует скрытое состояние из прошлой иттерации для создания нового. Это описывается с помощью формул:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
        "$$\n",
        "\n",
        "Напишем этот модуль на `Torch`!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBKQXeMOwie-"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Создай нужные параметры\n",
        "\n",
        "    def forward(self, x, hidden = None):\n",
        "        '''\n",
        "        x – torch.FloatTensor with the shape (bs, *, emb_size)\n",
        "        hidden - torch.FloatTensor with the shape (bs, hidden_size)\n",
        "        return: torch.FloatTensor with the shape (bs, hidden_size)\n",
        "        '''\n",
        "        hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device) if hidden is None else hidden\n",
        "        # Получи конечный hidden\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LJ3ZIuZzamP"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = ...\n",
        "        self.cls = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.rnn(x)\n",
        "        output = self.cls(hidden)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftuDhqdtzaj-"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = RNNModel(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 1\n",
        "max_grad_norm = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx62ofTU3CNY"
      },
      "source": [
        "training(model, criterion, optimizer, num_epochs, max_grad_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJqh9eWIhxE0"
      },
      "source": [
        "## GRU\n",
        "\n",
        "Третья модель: GRU. Она усложненная версия `RNN`. Гланая идея GRU: гейты. Так реализуется \"память\" модели – она маскирует часть старого скрытого состояния, создавая на этом месте новое. Модель GRU описывается следующим образом:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "            r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
        "            z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
        "            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
        "            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
        "        \\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi1yv2cy3CJB"
      },
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Создай нужные параметры\n",
        "\n",
        "    def forward(self, x, hidden = None):\n",
        "        '''\n",
        "        x – torch.FloatTensor with the shape (bs, *, emb_size)\n",
        "        hidden - torch.FloatTensro with the shape (bs, hidden_size)\n",
        "        return: torch.FloatTensor with the shape (bs, hidden_size)\n",
        "        '''\n",
        "        hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device) if hidden is None else hidden\n",
        "        # Получи конечный hidden\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bisyVic-3CDu"
      },
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gru = ...\n",
        "        self.cls = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.gru(x)\n",
        "        output = self.cls(hidden)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UASjrUtDzaf3"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = GRUModel(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 1\n",
        "max_grad_norm = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlom9sHdzac6"
      },
      "source": [
        "training(model, criterion, optimizer, num_epochs, max_grad_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG4My7G5i5Vs"
      },
      "source": [
        "## GRU + Embeddings\n",
        "\n",
        "Пока что мы фиксировали эмбеддинги в наших моделях. Но их можно доучить на наших данных! Для этого надо немного переделать способ подачи данных в модель и добавить в модель модуль `Embedding`. По-экспериментируем на модели `GRU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWNgnj98CC_v"
      },
      "source": [
        "class AgNewsDatasetv2(Dataset):\n",
        "    def __init__(self, train=True, max_length=128):\n",
        "        self.data = dataset[\"train\"] if train else dataset[\"test\"]\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        self.max_length = max_length\n",
        "        self.vocab = ... # Получи все токены из текста\n",
        "        self.PAD = 0\n",
        "        self.word2idx = ... # Сделай словарь, переводящий слова в индекс. не забудь про токен PAD\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[item][\"text\"]\n",
        "        tokens = self.tokenizer.tokenize(text.lower())\n",
        "        embeds = [self.word2idx.get(token, self.PAD) for token in tokens][:self.max_length]\n",
        "        return {\"inputs\": embeds, \"label\": self.data[item][\"label\"]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbt9HGnXLVx0"
      },
      "source": [
        "def collate_fn_v2(batch):\n",
        "    max_len = max(len(row[\"inputs\"]) for row in batch)\n",
        "    input_embeds = np.zeros((len(batch), max_len))\n",
        "    labels = np.zeros((len(batch),))\n",
        "    for idx, row in enumerate(batch):\n",
        "        input_embeds[idx][:len(row[\"inputs\"])] += row[\"inputs\"]\n",
        "        labels[idx] = row[\"label\"]\n",
        "    return {\"inputs\": torch.LongTensor(input_embeds), \"labels\": torch.LongTensor(labels)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDdCVPWLVsJ"
      },
      "source": [
        "train_dataset = AgNewsDatasetv2(train=True)\n",
        "valid_dataset = AgNewsDatasetv2(train=False)\n",
        "valid_dataset.vocab = train_dataset.vocab\n",
        "valid_dataset.word2idx = train_dataset.word2idx\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_sampler=TextSampler(train_sampler), collate_fn=collate_fn_v2, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_sampler=TextSampler(valid_sampler), collate_fn=collate_fn_v2, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OUbKpMyLVnN"
      },
      "source": [
        "class GRUModelv2(nn.Module):\n",
        "    def __init__(self, voc_size, embed_size, hidden_size, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = ...\n",
        "        self.gru = ...\n",
        "        self.cls = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        hidden = self.gru(x)\n",
        "        output = self.cls(hidden)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbYLzLdYLxjc"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = GRUModelv2(len(train_dataset.vocab) + 1, word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 1\n",
        "max_grad_norm = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-TxEwzMLsuz"
      },
      "source": [
        "with torch.no_grad():\n",
        "    # Воспользуйся предобученными эмбеддингами из glove\n",
        "    # для инициализации эмбеддингов модели"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opw85y82Mhvf"
      },
      "source": [
        "training(model, criterion, optimizer, num_epochs, max_grad_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trrbDTJqMvLK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}