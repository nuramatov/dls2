{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUI02UFXY5lZ"
   },
   "source": [
    "# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RNN. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–ü—Ä–∏–≤–µ—Ç! –í —ç—Ç–æ —Å–µ–º–∏–Ω–∞—Ä–µ –º—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–º—Å—è —Å –∑–∞–¥–∞—á–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–∏—Å–∫–∞ —Ç–µ–º–∞—Ç–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ —Å –¥–≤—É–º—è –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π ‚Äì¬†RNN –∏ GRU.\n",
    "\n",
    "–ù–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ—Ç `HuggingFaceü§ó` –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º `datasets`. –û–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eSlBJXbpNhH"
   },
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwNeXLCXr4ue"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBGF3mNQKAgN"
   },
   "outputs": [],
   "source": [
    "# –ó–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º!\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX-nNNuqZ9GN"
   },
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π: `AgNews`. –í –Ω–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω—ã —Ç–µ–∫—Å—Ç—ã –Ω–∞ 4 —Ç–µ–º—ã: `World`, `Sports`, `Business`, `Sci/Tech`. –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LREB3dpscnH"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"ag_news\")\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YysP5HpsiBX"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QK1tDYKaTJa"
   },
   "source": [
    "–í `dataset` –Ω–∞—Ö–æ–¥—è—Ç—Å—è `train` –∏ `test` —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjEB-Yv09AQo"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSIXBJPaegW"
   },
   "source": [
    "–ß—Ç–æ–±—ã –ø—Ä–µ–≤—Ä–∞—â–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–±–æ—Ä–∞ —Å–ª–æ–≤ –≤ –Ω–∞–±–æ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏—Ö —Å–ø–∏—Å–æ–∫ –∏ –≤—ã–±–µ—Ä–µ–º –æ–¥–∏–Ω –∏–∑ –Ω–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGW4jSiswVDy"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd1RzPKhwC3q"
   },
   "outputs": [],
   "source": [
    "word2vec = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAIznT0Kaue8"
   },
   "source": [
    "–û–±—ä–µ–¥–µ–Ω–∏–º `dataset`, `word2vec` –≤ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –º–∞—Å—Å–∏–≤ –∏–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∫–ª–∞—Å—Å —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdVbtWJzszbu"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class AgNewsDataset(Dataset):\n",
    "    def __init__(self, word2vec, train=True, max_length=128):\n",
    "        self.data = dataset[\"train\"] if train else dataset[\"test\"]\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        self.word2vec = word2vec\n",
    "        self.max_length = max_length # –õ—É—á—à–µ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[item][\"text\"]\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        embeds = [\n",
    "            self.word2vec.get_vector(token) \n",
    "            for token in tokens if token in self.word2vec\n",
    "        ][:self.max_length]\n",
    "        return {\"inputs\": (np.array(embeds) - self.mean) / self.std, \"label\": self.data[item][\"label\"]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-8eQDPJbIJl"
   },
   "source": [
    "–û–±—ã—á–Ω–æ –¥–ª—è —Å–±–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –±–∞—Ç—á–∏ –ø—Ä–æ—Å—Ç–æ —Å–æ–±–∏—Ä–∞—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤. –í —Å–ª—É—á–∞–µ —Ç–µ–∫—Å—Ç–æ–≤ —ç—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ‚Äì –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä. –ú–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω—É–ª—è–º–∏, –Ω–æ —ç—Ç–æ –Ω–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ. –ê –º–æ–∂–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç—ã –≤ –±–∞—Ç—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –¥–ª–∏–Ω. –¢–æ–≥–¥–∞ –±—É–¥–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ, –∏ –ø–æ–∫–∞ —ç—Ç–æ —á–∏—Å–ª–æ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–≤—ã—à–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –≤ –±–∞—Ç—á.\n",
    "\n",
    "–î–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞–¥–æ —Ä–∞—Å—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã –ø–æ –¥–ª–∏–Ω–∞–º. –ù–æ –ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ç–æ–≥–æ –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ(–±–∞—Ç—á–∏ –Ω–µ –±—É–¥—É—Ç —Å–ª—É—á–∞–π–Ω—ã). –ü–æ—ç—Ç–æ–º—É –Ω–∞ —Å–µ–º–∏–Ω–∞—Ä–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwDQaeh7s4tF"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler, RandomSampler\n",
    "\n",
    "\n",
    "class TextSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size_tokens=1e5):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size_tokens = batch_size_tokens\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        max_len = 0\n",
    "        for ix in self.sampler:\n",
    "            row = self.sampler.data_source[ix]\n",
    "            max_len = max(max_len, len(row[\"inputs\"]))\n",
    "            if (len(batch) + 1) * max_len > self.batch_size_tokens:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                max_len = len(row[\"inputs\"])\n",
    "            batch.append(ix)\n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzwuahwBxb2l"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_len = max(len(row[\"inputs\"]) for row in batch)\n",
    "    input_embeds = np.zeros((len(batch), max_len, word2vec.vector_size))\n",
    "    labels = np.zeros((len(batch),))\n",
    "    for idx, row in enumerate(batch):\n",
    "        input_embeds[idx][:len(row[\"inputs\"])] += row[\"inputs\"]\n",
    "        labels[idx] = row[\"label\"]\n",
    "    return {\"inputs\": torch.FloatTensor(input_embeds), \"labels\": torch.LongTensor(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YziKZLiUwva0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "\n",
    "train_dataset = AgNewsDataset(word2vec, train=True)\n",
    "valid_dataset = AgNewsDataset(word2vec, train=False)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=TextSampler(train_sampler), collate_fn=collate_fn, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=TextSampler(valid_sampler), collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbsqpZ-WfPqe"
   },
   "source": [
    "## CNN\n",
    "\n",
    "–ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º: CNN. –û–¥–Ω–æ–º–µ—Ä–Ω–∞—è –∫–æ–Ω–≤–æ–ª—é—Ü–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –í –∫–æ–Ω—Ü–µ –Ω–∞–¥–æ —Å–æ–±—Ä–∞—Ç—å –≤–µ–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é `AdaptiveMaxPool1d` –∏–ª–∏ `AdiptiveAvgPool1d`. –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –ª—é–±—É—é Feed Forward Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9DMopWCiBlD"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(embed_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        features = self.cnn(x)\n",
    "        prediction = self.cl(features)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ed-3nQHjDrd"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CNNModel(word2vec.vector_size, 50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oZjYEBkgauj"
   },
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7Smd5_03CSp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def training(model, criterion, optimizer, num_epochs, max_grad_norm=0):\n",
    "    for e in range(num_epochs):\n",
    "        model.train()\n",
    "        num_iter = 0\n",
    "        pbar = tqdm(train_loader)\n",
    "        for batch in pbar:\n",
    "            input_embeds = batch[\"inputs\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(input_embeds)\n",
    "            loss = criterion(prediction, labels)\n",
    "            loss.backward()\n",
    "            pbar.update(labels.size(0))\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            num_iter += 1\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        num_iter = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_embeds = batch[\"inputs\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                prediction = model(input_embeds)\n",
    "                valid_loss += criterion(prediction, labels)\n",
    "                valid_acc += (labels == torch.max(prediction, axis=1)[1]).float().mean()\n",
    "                num_iter += 1\n",
    "\n",
    "        print(f\"Valid Loss: {valid_loss / num_iter}, accuracy: {valid_acc / num_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOi77V2XjDpg"
   },
   "outputs": [],
   "source": [
    "training(model, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN8E8-ZSgnul"
   },
   "source": [
    "## RNN\n",
    "\n",
    "–í—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å: RNN. –≠—Ç–æ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å, –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ –ø—Ä–æ—à–ª–æ–π –∏—Ç—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ. –≠—Ç–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ñ–æ—Ä–º—É–ª:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
    "$$\n",
    "\n",
    "–ù–∞–ø–∏—à–µ–º —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –Ω–∞ `Torch`!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBKQXeMOwie-"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.w_h = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.rand((1, hidden_size)))\n",
    "        self.w_x = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_x = nn.Parameter(torch.rand(1, hidden_size))\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        '''\n",
    "        x ‚Äì torch.FloatTensor with the shape (bs, *, emb_size)\n",
    "        hidden - torch.FloatTensro with the shape (bs, hidden_size)\n",
    "        return: torch.FloatTensor with the shape (bs, hidden_size)\n",
    "        '''\n",
    "        hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device) if hidden is None else hidden\n",
    "        for idx in range(x.size(1)):\n",
    "            hidden = torch.tanh(x[:, idx] @ self.w_x + self.b_x + hidden @ self.w_h + self.b_h)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LJ3ZIuZzamP"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = RNN(embed_size, hidden_size)\n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.rnn(x)\n",
    "        output = self.cls(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftuDhqdtzaj-"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = RNNModel(word2vec.vector_size, 50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "num_epochs = 1\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yx62ofTU3CNY"
   },
   "outputs": [],
   "source": [
    "training(model, criterion, optimizer, num_epochs, max_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJqh9eWIhxE0"
   },
   "source": [
    "## GRU\n",
    "\n",
    "–¢—Ä–µ—Ç—å—è –º–æ–¥–µ–ª—å: GRU. –û–Ω–∞ —É—Å–ª–æ–∂–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è `RNN`. –ì–ª–∞–Ω–∞—è –∏–¥–µ—è GRU: –≥–µ–π—Ç—ã. –¢–∞–∫ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è \"–ø–∞–º—è—Ç—å\" –º–æ–¥–µ–ª–∏ ‚Äì –æ–Ω–∞ –º–∞—Å–∫–∏—Ä—É–µ—Ç —á–∞—Å—Ç—å —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, —Å–æ–∑–¥–∞–≤–∞—è –Ω–∞ —ç—Ç–æ–º –º–µ—Å—Ç–µ –Ω–æ–≤–æ–µ. –ú–æ–¥–µ–ª—å GRU –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "            r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "            z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "            n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
    "        \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi1yv2cy3CJB"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.w_rh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_rh = nn.Parameter(torch.rand((1, hidden_size)))\n",
    "        self.w_rx = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_rx = nn.Parameter(torch.rand(1, hidden_size))\n",
    "\n",
    "        self.w_zh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_zh = nn.Parameter(torch.rand((1, hidden_size)))\n",
    "        self.w_zx = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_zx = nn.Parameter(torch.rand(1, hidden_size))\n",
    "\n",
    "        self.w_nh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_nh = nn.Parameter(torch.rand((1, hidden_size)))\n",
    "        self.w_nx = nn.Parameter(torch.rand(embed_size, hidden_size))\n",
    "        self.b_nx = nn.Parameter(torch.rand(1, hidden_size))\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        '''\n",
    "        x ‚Äì torch.FloatTensor with the shape (bs, *, emb_size)\n",
    "        hidden - torch.FloatTensro with the shape (bs, hidden_size)\n",
    "        return: torch.FloatTensor with the shape (bs, hidden_size)\n",
    "        '''\n",
    "        hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device) if hidden is None else hidden\n",
    "        for idx in range(x.size(1)):\n",
    "            r = torch.sigmoid(x[:, idx] @ self.w_rx + self.b_rx + hidden @ self.w_rh + self.b_rh)\n",
    "            z = torch.sigmoid(x[:, idx] @ self.w_zx + self.b_zx + hidden @ self.w_zh + self.b_zh)\n",
    "            n = torch.tanh(x[:, idx] @ self.w_zx + self.b_zx + r * (hidden @ self.w_zh + self.b_zh))\n",
    "            hidden = (1 - z) * n + z * hidden\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bisyVic-3CDu"
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gru = GRU(embed_size, hidden_size)\n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.gru(x)\n",
    "        output = self.cls(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UASjrUtDzaf3"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GRUModel(word2vec.vector_size, 50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "num_epochs = 1\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlom9sHdzac6"
   },
   "outputs": [],
   "source": [
    "training(model, criterion, optimizer, num_epochs, max_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG4My7G5i5Vs"
   },
   "source": [
    "## GRU + Embeddings\n",
    "\n",
    "–ü–æ–∫–∞ —á—Ç–æ –º—ã —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª—è—Ö. –ù–æ –∏—Ö –º–æ–∂–Ω–æ –¥–æ—É—á–∏—Ç—å –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö! –î–ª—è —ç—Ç–æ–≥–æ –Ω–∞–¥–æ –Ω–µ–º–Ω–æ–≥–æ –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å —Å–ø–æ—Å–æ–± –ø–æ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –º–æ–¥–µ–ª—å –∏ –¥–æ–±–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å –º–æ–¥—É–ª—å `Embedding`. –ü–æ-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–µ–º –Ω–∞ –º–æ–¥–µ–ª–∏ `GRU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWNgnj98CC_v"
   },
   "outputs": [],
   "source": [
    "class AgNewsDatasetv2(Dataset):\n",
    "    def __init__(self, train=True, max_length=128):\n",
    "        self.data = dataset[\"train\"] if train else dataset[\"test\"]\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        self.max_length = max_length\n",
    "        self.vocab = set(\n",
    "            self.tokenizer.tokenize(\"\".join(d[\"text\"].lower() for d in self.data))\n",
    "        )\n",
    "        self.word2idx = {word: idx + 1 for idx, word in enumerate(self.vocab)}\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[item][\"text\"]\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        embeds = [self.word2idx.get(token, 0) for token in tokens][:self.max_length]\n",
    "        return {\"inputs\": embeds, \"label\": self.data[item][\"label\"]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbt9HGnXLVx0"
   },
   "outputs": [],
   "source": [
    "def collate_fn_v2(batch):\n",
    "    max_len = max(len(row[\"inputs\"]) for row in batch)\n",
    "    input_embeds = np.zeros((len(batch), max_len))\n",
    "    labels = np.zeros((len(batch),))\n",
    "    for idx, row in enumerate(batch):\n",
    "        input_embeds[idx][:len(row[\"inputs\"])] += row[\"inputs\"]\n",
    "        labels[idx] = row[\"label\"]\n",
    "    return {\"inputs\": torch.LongTensor(input_embeds), \"labels\": torch.LongTensor(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyDdCVPWLVsJ"
   },
   "outputs": [],
   "source": [
    "train_dataset = AgNewsDatasetv2(train=True)\n",
    "valid_dataset = AgNewsDatasetv2(train=False)\n",
    "valid_dataset.vocab = train_dataset.vocab\n",
    "valid_dataset.word2idx = train_dataset.word2idx\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=TextSampler(train_sampler), collate_fn=collate_fn_v2, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=TextSampler(valid_sampler), collate_fn=collate_fn_v2, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OUbKpMyLVnN"
   },
   "outputs": [],
   "source": [
    "class GRUModelv2(nn.Module):\n",
    "    def __init__(self, voc_size, embed_size, hidden_size, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(voc_size, embed_size)\n",
    "        self.gru = GRU(embed_size, hidden_size)\n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        hidden = self.gru(x)\n",
    "        output = self.cls(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbYLzLdYLxjc"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GRUModelv2(len(train_dataset.vocab) + 1, word2vec.vector_size, 50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "num_epochs = 1\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-TxEwzMLsuz"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for word in train_dataset.vocab:\n",
    "        if word in word2vec:\n",
    "            model.emb.weight[train_dataset.word2idx[word]] = torch.from_numpy(word2vec.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Opw85y82Mhvf"
   },
   "outputs": [],
   "source": [
    "training(model, criterion, optimizer, num_epochs, max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trrbDTJqMvLK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seminar_RNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
