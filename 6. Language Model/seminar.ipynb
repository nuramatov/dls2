{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk gdown seaborn torchtext pymorphy2 gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар: Language Model\n",
    "\n",
    "Привет! Сегодня мы создадим свою Language Model! Посмотрим на три вида моделей: N-gram, CNN, LSTM. Для обучения LM лучше всего подходят большие корпуса с разнообразными текстами: от новостей до художственной литературы. Для русского языка есть большой корпус [Taiga](https://tatianashavrina.github.io/taiga_site/). Для английского используют тексты из [википедии](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) или [BookCorpus](https://github.com/soskek/bookcorpus). \n",
    "\n",
    "Сегодня вы возьмем маленькую часть датасета Taiga: новости с сайта [nplus1](https://nplus1.ru). Каждая новость на сайте помечается меткой сложности (от 0 до 10). Это не поможет нам с обучением хорошей LM, но даст возможность поиграться с генерацией текста.\n",
    "\n",
    "Загрузим датасет и подготовим его к работе!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "gdown.download(\"https://drive.google.com/uc?id=1UtF9urwAL2OiMg7N5iFmZmeiRzq1Psw6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip nplus1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nplus1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся информация про тексты содержится в таблице `newmetadata.csv`. Загрузим её с помощью `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "metadata = pd.read_table(\"nplus1/newmetadata.csv\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонка `textdiff` содержит информацию про сложность текста. Чтобы выделить нужный кусок, воспользуемся методами `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[(metadata[\"textdiff\"] > 4) & (metadata[\"textdiff\"] < 5)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение сложности текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.set()\n",
    "sns.histplot(metadata[\"textdiff\"], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим предобученные эмбеддинги, которые готовы к работе с русским языком ([весь список](https://rusvectores.org/ru/models/)). Из-за особенностей русского языка эмбеддинги ожидают строку вида `{слово}_{часть речи}`. Надо про это помнить при работе с этими эмбеддингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader as api\n",
    "\n",
    "word2vec = api.load('word2vec-ruscorpora-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим датасет к работе с моделями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "EOS = \"<EOS>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, min_diff=0, max_diff=10):\n",
    "        self.root = Path(\"nplus1/texts\")\n",
    "        metadata = pd.read_table(\"nplus1/newmetadata.csv\")\n",
    "        self.metadata = ... # Получи нужную часть таблицы с помощью `min_diff` и `max_diff`\n",
    "        \n",
    "        # Получим список всех текстов и сверим его с таблицей\n",
    "        file_paths = np.array(list(self.root.glob(\"*.txt\")))\n",
    "        text_ids = np.array(list(path.name.split(\".\")[0] for path in file_paths))\n",
    "        self.text_ids = text_ids[np.isin(text_ids, self.metadata[\"textid\"])]\n",
    "        self.file_paths = file_paths[np.isin(text_ids, self.metadata[\"textid\"])]\n",
    "        \n",
    "        self.min_diff = min_diff\n",
    "        self.max_diff = max_diff\n",
    "        \n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        self.morph = MorphAnalyzer()\n",
    "        \n",
    "        self.token2idx = {PAD: 0, EOS: 1, UNK: 2}\n",
    "        self.vocab = set([PAD, EOS, UNK])\n",
    "        for path in tqdm(self.file_paths):\n",
    "            with open(path) as file:\n",
    "                text = ...# прочитай текст из файла\n",
    "                self.vocab.update(...) # добавь токены в словарь\n",
    "        self.token2idx.update({t:num + 3 for num, t in enumerate(vocab)})\n",
    "        self.idx2token = {num: token for token, num in self.token2idx.items()}\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        with open(self.file_paths[item]) as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        tokens = ... # с токенизируй текст\n",
    "        \n",
    "        text_id = self.text_ids[item]\n",
    "        textdiff = ... # получи сложность текста\n",
    "        \n",
    "        # для обучение нейронок нам потребуются индексы токенов в словаре.\n",
    "        input_ids = ... # получи их из self.token2idx\n",
    "        \n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"tokens\": tokens,\n",
    "            \"textdiff\": textdiff,\n",
    "            \"input_ids\": input_ids\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def tokenize_(self, text):\n",
    "        tokens = self.tokenizer.tokenize(text.lower())\n",
    "        morphs = [self.morph.parse(token)[0]\n",
    "                  for token in tokens \n",
    "                  if (token not in string.punctuation)]\n",
    "        tokens = [f\"{morph.normal_form}_{morph.tag.POS}\" for morph in morphs]\n",
    "        tokens = [token for token in tokens if token in word2vec]\n",
    "        tokens += [EOS]\n",
    "        return tokens\n",
    "    \n",
    "    def embeddins(self):\n",
    "        w = torch.rand(len(self.vocab), word2vec.vector_size)\n",
    "        for token, num in self.token2idx.items():\n",
    "            if token in word2vec:\n",
    "                w[num] = ... # получи эмбеддинг для токена\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(0.5, 2)\n",
    "train_size = np.ceil(len(dataset) * 0.8).astype(int)\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-gram LM\n",
    "\n",
    "Первая жертва – N-граммная модель. Она пишется скучно, но хорошо работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class NGramModel(object):\n",
    "    '''\n",
    "    Структура этой реализации n-граммной модели следующая:\n",
    "    self.ngrams – словарь, который на каждый (token_0, ..., token_(n-1)) – n-1 tuple из токенов\n",
    "        хранит частоту появления следующего токена. Для подсчета числа токенов воспользуемся\n",
    "        Counter\n",
    "    self.tokenize_func – функция токенизации текста. С её помощью будем получать токены.\n",
    "    '''\n",
    "    def __init__(self, n=2):\n",
    "        self.ngrams = defaultdict(Counter)\n",
    "        self.n = n\n",
    "        self.tokenize_func = None\n",
    "        \n",
    "    def compute_ngrams(self, dataset, tokenize_func):\n",
    "        self.tokenize_func = tokenize_func\n",
    "        self.ngrams = defaultdict(Counter)\n",
    "        for row in tqdm(dataset):\n",
    "            ngram = [PAD] * self.n\n",
    "            for token in row[\"tokens\"]:\n",
    "                ... # обнови self.ngram новыми токенами\n",
    "            \n",
    "    def get_log_probs(self, prefix, min_log_pr=-15):\n",
    "        '''\n",
    "        Функция, которая будет возвращать логарифмы частот появления токенов\n",
    "        '''\n",
    "        if isinstance(prefix, str):\n",
    "            # преврати строку в tuple из токенов с помощью tokenize_func. \n",
    "            prefix = ... # не забывай, что tokenize_func добавляет <EOS>\n",
    "        if len(prefix) < self.n - 1:\n",
    "            prefix = [PAD] * (self.n - len(prefix) - 1) + prefix\n",
    "        else:\n",
    "            prefix = prefix[-self.n + 1:]\n",
    "        possible_ends = ... # получи количество появление токенов с таким префиксом\n",
    "        sum_freq = ... # получи количество появление префикса в текстах\n",
    "        return ... # верни логарифм частоты появления токенов\n",
    "    \n",
    "    def sample(self, prefix):\n",
    "        possible_ends = self.get_log_probs(prefix)\n",
    "        if len(possible_ends) > 0:\n",
    "            end = np.random.choice(list(possible_ends.keys()), p=np.exp(list(possible_ends.values())))\n",
    "            return end\n",
    "        return EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим 5-граммную модель и посмотрим, как хорошо справляется она с генерацией текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frigram = NGramModel(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frigram.compute_ngrams(train_dataset, dataset.tokenize_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frigram.get_log_probs(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frigram.sample(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prefix, lenth=100):\n",
    "    text = \"\" + prefix\n",
    "    while len(text) < lenth:\n",
    "        ... # получи новый токен по предыдущим. Добавь его в текст.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(frigram, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количественная величина, которая позволяет сравнивать LM: перплекция. Для её вычисления используется следующая формула:\n",
    "\n",
    "$$\n",
    "\\text{Ppr} = \\frac{1}{|D|} \\sum_{t \\in D}\\sum_{w \\in t} - \\log (p(w)),\n",
    "$$\n",
    "где $D$ – валидационный датасет, $|D|$ – общая длина текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_ngram(dataset, model):\n",
    "    lengths = 0\n",
    "    log_prob = 0\n",
    "    for row in tqdm(dataset):\n",
    "        ... # получи метрики для вычисление перплексии для текущей сторки\n",
    "    return np.exp(-log_prob / lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_ngram(valid_dataset, frigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NN LM\n",
    "\n",
    "Приступим к нейросетевым языковым моделям. Для начала нам потребуется сэмплер из прошлого семинара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class TextSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size_tokens=1e4):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size_tokens = batch_size_tokens\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        max_len = 0\n",
    "        for ix in self.sampler:\n",
    "            row = self.sampler.data_source[ix]\n",
    "            max_len = max(max_len, len(row[\"input_ids\"]))\n",
    "            if (len(batch) + 1) * max_len > self.batch_size_tokens:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                max_len = len(row[\"input_ids\"])\n",
    "            batch.append(ix)\n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_len = max(len(row[\"input_ids\"]) for row in batch)\n",
    "    input_embeds = np.zeros((len(batch), max_len))\n",
    "    for idx, row in enumerate(batch):\n",
    "        input_embeds[idx][:len(row[\"input_ids\"])] += row[\"input_ids\"]\n",
    "    row[\"input_ids\"] = torch.LongTensor(input_embeds)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, random_split\n",
    "\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=TextSampler(train_sampler), collate_fn=collate_fn, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=TextSampler(valid_sampler), collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "Вторая жертва – CNN. Если внимательно посмотреть, то она является нейросетевым приближением к n-грамной модели. Для её реализации нам потребуется новым модуль – `nn.ZeroPad2d`[docs](https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html). Он добавит нулей в нужном месте, чтобы конволюционный слой смотрел только на предыдущие токены при предсказании текущего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pad = ... # создай ZeroPad2d\n",
    "        self.conv = ... # создай однослойную конволюцию\n",
    "        self.pred = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embed = self.emb(input_ids).permute(0, 2, 1)\n",
    "        padded = self.pad(embed)\n",
    "        convolved = torch.relu(self.conv(padded)).permute(0, 2, 1)\n",
    "        return self.pred(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CNNLM(len(dataset.vocab), 300, 100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.token2idx[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.emb.weight.copy_(dataset.embeddins())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            ... # обучи модель \n",
    "\n",
    "            pbar.update(input_ids.size(0))\n",
    "        \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    n_iter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            n_iter += 1\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            prediction = model(input_ids[:, :-1])\n",
    "            valid_loss += criterion(prediction.reshape(-1, prediction.size(-1)), input_ids[:, 1:].reshape(-1))\n",
    "    print(f\"Valid Loss: {valid_loss / n_iter}, Valid Peprplexity: {torch.exp(valid_loss / n_iter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения модели посмотрим, как она справляется с задачей генерации текста. Сделаем специальную функцию для этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, prefix, max_length=100):\n",
    "    tokens = dataset.tokenize_(prefix)[:-1]\n",
    "    input_ids = [dataset.token2idx.get(token) for token in tokens]\n",
    "    input_ids_tensor = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            output = model(input_ids_tensor)\n",
    "\n",
    "            probs = ... # получи из output вероятности следующего токена\n",
    "            next_id = ... # получи следующий токен\n",
    "            tokens += ... # добавь токен в список токенов\n",
    "            \n",
    "            if dataset.idx2token[next_id] == EOS or len(tokens) > max_length:\n",
    "                break\n",
    "            input_ids += [next_id]\n",
    "            input_ids_tensor = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    return \" \".join(t.split(\"_\")[0] for t in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model, \"привет\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Последняя жертва – LSTM. Она должна лучше работать с длинными текстами, потому что у неё нет фиксированного количества токенов, на которые она можеть \"смотреть\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = ... # Сделай lstm слой\n",
    "        self.pred = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embs = self.emb(input_ids)\n",
    "        output, _ = self.lstm(embs)\n",
    "        return self.pred(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMLM(len(dataset.vocab), 300, 100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.token2idx[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.emb.weight.copy_(dataset.embeddins())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            ... # обучи модель \n",
    "\n",
    "            pbar.update(input_ids.size(0))\n",
    "        \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    n_iter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            n_iter += 1\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            prediction = model(input_ids[:, :-1])\n",
    "            valid_loss += criterion(prediction.reshape(-1, prediction.size(-1)), input_ids[:, 1:].reshape(-1))\n",
    "    print(f\"Valid Loss: {valid_loss / n_iter}, Valid Peprplexity: {torch.exp(valid_loss / n_iter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model, \"привет\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что дальше?\n",
    " \n",
    "Если мы говорим про генерацию, то модель надо обучать подольше и на большом количестве текста. Если хочешь поэкспериментировать с генерацией, то я предлагаю такой план:\n",
    " \n",
    "- обучи модель на всех новостях (min_diff=0, max_diff=10)\n",
    "- сохрани веса этой модели (torch.save(model.state_dict()))\n",
    "- переобучи несколько моделей на новостях с другими значениями сложности (eg. (min_diff=1, max_diff=3), (min_diff=4, max_diff=8))\n",
    "- сравни сгенерированные тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
