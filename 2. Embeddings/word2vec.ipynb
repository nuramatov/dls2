{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e93e93e975a74a66a32c24df0df4de64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9b40c91c11f14896a024f7b5bd576e76","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f62c874c633a4076b7d47dcc96ff001a","IPY_MODEL_f918e892c26643fc81331746d25b605e"]}},"9b40c91c11f14896a024f7b5bd576e76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f62c874c633a4076b7d47dcc96ff001a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5f754499845048bfaa398a1effd89ac2","_dom_classes":[],"description":"Creating context dataset:   0%","_model_name":"FloatProgressModel","bar_style":"danger","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ed6e5160906f40b096a4a2532ecd69d1"}},"f918e892c26643fc81331746d25b605e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8cbfdd99d00424abf6e840a00b6ee5c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1 [00:03&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d947f40436654672836c9320379ccda3"}},"5f754499845048bfaa398a1effd89ac2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ed6e5160906f40b096a4a2532ecd69d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8cbfdd99d00424abf6e840a00b6ee5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d947f40436654672836c9320379ccda3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c4cc2c123cd4ab09022f4eb669e92b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0587bde5f6ce46e8a9c52e501840ab19","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f1e0fa91ca614e6ea0431dd3fbae9ba1","IPY_MODEL_545c21c0eb6444daa94045e0a73cb25a"]}},"0587bde5f6ce46e8a9c52e501840ab19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f1e0fa91ca614e6ea0431dd3fbae9ba1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4213ba662a4b491a81b5d608829a6f50","_dom_classes":[],"description":"  1%","_model_name":"FloatProgressModel","bar_style":"danger","max":17005207,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":171392,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_223c4d0225434d1faf2512d8eb00501d"}},"545c21c0eb6444daa94045e0a73cb25a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_069bba65d6424839b1b8521ea069abcf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 171392/17005207 [00:19&lt;04:40, 60080.18it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9c97082b361043e08051702a6632a8cd"}},"4213ba662a4b491a81b5d608829a6f50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"223c4d0225434d1faf2512d8eb00501d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"069bba65d6424839b1b8521ea069abcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9c97082b361043e08051702a6632a8cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ITDRNpvV7CAP"},"source":["# Word to vec\n","\n","How do we understand meaning of previosl unseen word? We are searching for this word in a context."]},{"cell_type":"markdown","metadata":{"id":"OZm_ytDjBBD1"},"source":["## Theory\n","\n","There are two types of word to vec.\n","\n","### Skipgram\n","\n","Predicting outside word $o$ from central $c$. We have two embedding mattrix $u$ and $v$.\n","\n","$P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}$.\n","\n","![](https://i.ibb.co/xgT4k8b/2020-10-02-10-10-21.png)\n","\n","More formally we need to maximize Likelohood:\n","$$\n","L(\\theta)=\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P\\left(w_{t+j} \\mid w_{t}, \\theta\\right)\n","$$\n","\n","$$\n","L_{\\log}(\\theta) = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P\\left(w_{t+j} \\mid w_{t}, \\theta\\right) = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log \\frac{\\exp \\left(u_{t+j}^{T} v_{t}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{t}\\right)} = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} u_{t+j}^{T} v_{t} - \\log \\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{t}\\right)\n","$$\n","\n","$$\n","loss = -L_{\\log}\n","$$\n","\n","Let's count derivative!\n","\n","**Reminder**\n","\n","$$\\frac{\\partial x^T y}{\\partial y} = x$$\n","\n","Let $o = t+j$, for one step:\n","\n","$$\n","\\frac{\\partial L_{log}(\\theta)}{\\partial v_t} = u_o - \\dfrac{1}{\\sum_w\\exp(u_w^T v_t)}\\cdot\\sum_x \\exp(u_x^T v_t) u_x = u_o - \\sum_x \\frac{\\exp(u_x^T v_t)}{\\sum_w \\exp(u_w^T v_t)} u_x = \\\\ = u_0 - \\sum_x P(u_x| v_t) u_x\n","$$\n","\n","### CBOW\n","\n","![](https://lena-voita.github.io/resources/lectures/word_emb/w2v/cbow_skip-min.png)\n","\n","## Practice\n","\n","### Variant 1\n","\n","```python\n","class Model(nn.Module):\n","    def __init__(self, voc_size, emb_dim):\n","        self.u = nn.Embedding(voc_size, emb_dim)\n","        self.v = nn.Embedding(voc_size, emb_dim)\n","\n","w2v = Model(...)\n","\n","def step(word, context):\n","    for c_word in context:\n","        loss = - w2v.u(word).T.dot(w2v.v(c_word))\n","        cum_exp = 0\n","        for i in range(voc_size):\n","            if i == c_word:\n","                continue\n","            cum_exp += w2v.u(word).T.dot(w2v.v(c_word)).exp()\n","        loss += torch.log(cum_exp)\n","        loss.backward()\n","        ...\n","```\n","\n","### Variant 2\n","\n","![](https://i.ibb.co/qydjBbv/2020-10-02-12-16-33.png)\n","\n","```python\n","class Model(nn.Module):\n","    def __init__(self, voc_size, emb_dim):\n","        self.u = nn.Embedding(voc_size, emb_dim)\n","        self.v = nn.Linear(emb_dim, voc_size, bias=False)\n","\n","    def forward(self, x):\n","        return self.v(self.u(x))\n","\n","w2v = Model(...)\n","criterion = nn.CrossEntropyLoss()\n","\n","def step(word, context):\n","    for c_word in context:\n","        preds = w2v(word)\n","        loss = criterion(preds, c_word)\n","        loss.backward()\n","        ...\n","```\n"]},{"cell_type":"code","metadata":{"id":"FECQTqty7td-","outputId":"9100e02c-3bde-4fe0-f3f7-7c5d6012cfb4","colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["!curl -O http://mattmahoney.net/dc/text8.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 29.8M  100 29.8M    0     0   694k      0  0:00:44  0:00:44 --:--:--  712k\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pHRGir7V71Vd","outputId":"d220b178-89cd-465d-85a3-bdf393e1a3c3","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["!unzip text8.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  text8.zip\n","  inflating: text8                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V9nuiVHTjlps","outputId":"28671ead-b559-4c7e-9764-d8106feb6cd2","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install -q catalyst"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 460kB 9.3MB/s \n","\u001b[K     |████████████████████████████████| 317kB 23.7MB/s \n","\u001b[K     |████████████████████████████████| 163kB 23.9MB/s \n","\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GNlABcjDdKEU"},"source":["import re\n","from collections import Counter\n","from tqdm.notebook import tqdm\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from catalyst import dl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBw9uOXXybb0"},"source":["\n","\n","class W2VCorpus:\n","    def __init__(\n","        self, path, voc_max_size: int = 40000, min_word_freq: int = 20, max_corp_size=5e6\n","    ):\n","        corpus = []\n","        sentences = []\n","        with open(path, \"r\") as inp:\n","            for line in inp:\n","                corpus.append(line.split())\n","                sentences.append(line)\n","        corpus = np.array(corpus)\n","        self.corpus = corpus\n","        most_freq_word = \\\n","            Counter(' '.join(sentences).split()).most_common(voc_max_size)\n","        most_freq_word = np.array(most_freq_word)\n","        most_freq_word = \\\n","            most_freq_word[most_freq_word[:, 1].astype(int) > min_word_freq]\n","        \n","        print('Vocabulary size is:' + str(len(most_freq_word)))\n","        self.vocabulary = set(most_freq_word[:, 0])\n","        self.vocabulary.update([\"<PAD>\"])\n","        self.vocabulary.update([\"<UNK>\"])\n","        self.word_freq = most_freq_word\n","        self.idx_to_word = dict(list(enumerate(self.vocabulary)))\n","        self.word_to_idx = \\\n","            dict([(i[1], i[0]) for i in enumerate(self.vocabulary)])\n","        self.W = None\n","        self.P = None\n","        self.positive_pairs = None\n","        \n","    def make_positive_dataset(self, window_size=2):\n","        \"\"\"take corpus and make positive examples for skipgram or CBOW\n","           like: [1234], [[3333, 1111, 2222, 4444]]\"\"\"\n","        if not self.W is None:\n","            return self.W, self.P\n","        W = []\n","        P = []\n","        pbar = tqdm(self.corpus)\n","        pbar.set_description('Creating context dataset')\n","        for message in pbar:\n","\n","            if len(self.corpus) == 1:\n","                iter_ = tqdm(enumerate(message), total=len(message))\n","            else:\n","                iter_ = enumerate(message)\n","            \n","            for idx, word in iter_:\n","                if word not in self.vocabulary:\n","                    word = \"<UNK>\"\n","                start_idx = max(0, idx - window_size)\n","                end_idx = min(len(message), idx+window_size+1)\n","                pos_in_window = window_size\n","                if idx - window_size < 0:  # start of the sentence\n","                    pos_in_window += idx - window_size\n","                    \n","                co_words = message[start_idx:end_idx]\n","                co_words = np.delete(co_words, pos_in_window)\n","                filtered_co_words = []\n","                \n","                for co_word in co_words:\n","                    if co_word in self.vocabulary:\n","                        filtered_co_words.append(co_word)\n","                    else:\n","                        filtered_co_words.append(\"<UNK>\")\n","                while len(filtered_co_words) < 2*window_size:\n","                    filtered_co_words.append(\"<PAD>\")\n","                W.append(self.word_to_idx[word])\n","                co_word_idx = [self.word_to_idx[co_word] for co_word in filtered_co_words]\n","                P.append(co_word_idx)\n","        self.W = W\n","        self.P = P\n","        del self.corpus\n","        return W, P\n","    \n","    def make_positive_pairs(self):\n","        if not self.positive_pairs is None:\n","            return self.positive_pairs\n","        if self.W is None:\n","            self.make_positive_dataset()\n","        pairs = []\n","        pbar = tqdm(zip(self.W, self.P), total=len(self.W))\n","        pbar.set_description('Creating positive pairs')\n","        for w, p in pbar:\n","            for cur_p in p:\n","                if cur_p != self.word_to_idx[\"<PAD>\"]:  # pad\n","                    pairs.append([w, cur_p])\n","        self.positive_pairs = pairs\n","        return pairs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grDGsNDjGegc","outputId":"5fd0bf24-28bf-40b8-ee45-6c2febc8f19f","colab":{"base_uri":"https://localhost:8080/","height":753,"referenced_widgets":["e93e93e975a74a66a32c24df0df4de64","9b40c91c11f14896a024f7b5bd576e76","f62c874c633a4076b7d47dcc96ff001a","f918e892c26643fc81331746d25b605e","5f754499845048bfaa398a1effd89ac2","ed6e5160906f40b096a4a2532ecd69d1","b8cbfdd99d00424abf6e840a00b6ee5c","d947f40436654672836c9320379ccda3","8c4cc2c123cd4ab09022f4eb669e92b6","0587bde5f6ce46e8a9c52e501840ab19","f1e0fa91ca614e6ea0431dd3fbae9ba1","545c21c0eb6444daa94045e0a73cb25a","4213ba662a4b491a81b5d608829a6f50","223c4d0225434d1faf2512d8eb00501d","069bba65d6424839b1b8521ea069abcf","9c97082b361043e08051702a6632a8cd"]}},"source":["corp = W2VCorpus(\"text8\")\n","\n","pairs = corp.make_positive_pairs()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocabulary size is:30964\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e93e93e975a74a66a32c24df0df4de64","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c4cc2c123cd4ab09022f4eb669e92b6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=17005207.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e247ee73a8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2VCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_positive_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-f7673f5cc39e>\u001b[0m in \u001b[0;36mmake_positive_pairs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_positive_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-f7673f5cc39e>\u001b[0m in \u001b[0;36mmake_positive_dataset\u001b[0;34m(self, window_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mco_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mco_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_in_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mfiltered_co_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4383\u001b[0m         \u001b[0mslobj2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4384\u001b[0m         \u001b[0mslobj2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4385\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"eXdfUDi6HG1f"},"source":["class W2VDataset(Dataset):\n","    def __init__(self, pairs):\n","        self.pairs = pairs\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"word\": torch.tensor(self.pairs[idx][0]),\n","            \"context\": torch.tensor(self.pairs[idx][1])\n","        }\n","\n","    def __len__(self):\n","        return len(self.pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UevtZSFOKGyY"},"source":["train_ds = W2VDataset(pairs)\n","train_dl = DataLoader(train_ds, batch_size=2048)\n","loaders = {\"train\": train_dl}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1kylxeJK3Kf"},"source":["class W2VModel(nn.Module):\n","    def __init__(self, voc_size, emb_dim):\n","        super().__init__()\n","        self.encoder = nn.Embedding(voc_size, emb_dim)\n","        self.decoder = nn.Linear(emb_dim, voc_size, bias=False)\n","        self.voc_size = voc_size\n","        self.emb_dim = emb_dim\n","        self.init_emb()\n","\n","    def forward(self, word):\n","        return self.decoder(self.encoder(word))\n","\n","    def init_emb(self):\n","        \"\"\"\n","        init the weight as original word2vec do.\n","        \"\"\"\n","        initrange = 0.5 / self.emb_dim\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.weight.data.uniform_(0, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJaZ8MYY19Cp"},"source":["%reload_ext tensorboard\n","%tensorboard --logdir ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4x-JvTFKMewU"},"source":["from catalyst import dl\n","\n","model = W2VModel(len(corp.vocabulary), 300)\n","runner = dl.SupervisedRunner(\n","    input_key=[\"word\"], input_target_key=[\"context\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZb_Xk2Q2POJ"},"source":["runner.train(\n","    model=model,\n","    optimizer=torch.optim.Adam(model.parameters()),\n","    loaders=loaders,\n","    criterion=nn.CrossEntropyLoss(),\n","    callbacks = [dl.CriterionCallback(input_key=\"context\")],\n","    num_epochs=1,\n","    logdir=\"simple_w2v_1\",\n","    verbose=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zyCLkEixcOV4"},"source":["## More tricks\n","\n","### Negative sampling\n","\n","Instead of updating all context vectors we can sample 5-10.\n","\n","$$\n","loss = -\\log \\sigma\\left(u_{context}^{T} v_{center}\\right)-\\sum_{w \\in\\left\\{w_{i_{1}}, \\ldots, w_{i_{K}}\\right\\}} \\log \\sigma\\left(-u_{w}^{T} v_{center}\\right)\n","$$\n","\n","**How can we sample negative words?**\n","\n","According to their probability: $p_{sample} (w) = p_{word} (w)$? Why not?\n","\n","Information $= -\\log(P)$\n","\n","\n","### Not all of the words are equally important\n","\n","$$\n","P\\left(w_{i}\\right)=1-\\sqrt{\\frac{t h r}{f\\left(w_{i}\\right)}}\n","$$\n","\n","$thr \\approx 10^-5$.\n","\n","### Distance between center and context\n","\n","![](https://lena-voita.github.io/resources/lectures/word_emb/research/w2v_position-min.png)\n"]},{"cell_type":"markdown","metadata":{"id":"JMwEjCVzzX1G"},"source":["### Default settings\n","\n","**Method:** Skipgram\n","\n","**Negative sample size**: about 5 for big dataset, 10-20 for small\n","\n","**Embedding space dim:** 300 (the quality remains the same for higher dims)\n","\n","**Window size**: 5-10"]},{"cell_type":"code","metadata":{"id":"TiNgjv9RCULr"},"source":["# in case you want to try to create embeddings yourself\n","\n","all_embeddings = []\n","all_words = []\n","\n","\n","for word, idx in corp.word_to_idx.items():\n","    with torch.no_grad():\n","        current_emb = model.embedding(torch.tensor(idx).to(device))\n","        current_emb = current_emb.cpu().detach().numpy()\n","        all_embeddings.append(current_emb)\n","        all_words.append(word)\n","all_embeddings = np.array(all_embeddings)\n","all_words = np.array(all_words).astype(str)\n","np.savetxt(\"embeddings_t8.tsv\", all_embeddings[:5000], delimiter=\"\\t\")\n","\n","with open(\"words_t8.tsv\", 'w') as out:\n","    for word in all_words[:5000]:\n","        out.write(word + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Zwn5Kwql9VZ"},"source":["# Embedding space\n","\n","[Pre-trained model embedding space](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/elephantmipt/4a46fe320b4eadf6ba47f0c073968244/raw/2489cf5c83032a19ecb4e38c8e51b779a1e5dc59/configs_t8.json)"]},{"cell_type":"code","metadata":{"id":"XQQLPOEyNgpZ"},"source":["import gensim.downloader as api\n","\n","model = api.load('word2vec-google-news-300')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sC9XB0Tl33R"},"source":["model.most_similar(\"door\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flDHwBGAyW2x"},"source":["### Syntactic\n","\n","$v_{kings} - v_{king} + v_{queen} \\approx v_{queens}$"]},{"cell_type":"code","metadata":{"id":"NyTIJo-Xyql_"},"source":["model.most_similar(positive=['kings', 'queen'], negative=['king'], topn=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FXX97fUcr9Vu"},"source":["### Semantic\n","$v_{king} - v_{man} + v_{woman} \\approx v_{queen}$"]},{"cell_type":"code","metadata":{"id":"XiwLXj_zmiq9"},"source":["model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWKy95dGwDtE"},"source":["model.most_similar(positive=['USA', 'vodka'], negative=['Russia'], topn=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u69-5zLev6HA"},"source":["## Bias"]},{"cell_type":"code","metadata":{"id":"8oDMlH7duKC8"},"source":["model.most_similar(positive=['woman', 'director'], negative=['man'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXfOEpt7wwiO"},"source":["President"]},{"cell_type":"code","metadata":{"id":"nhz8rikauUf_"},"source":["model.similarity(\"president\", \"man\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekbxDFM9vsul"},"source":["model.similarity(\"president\", \"woman\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccuAyZl7wyy3"},"source":["Crime"]},{"cell_type":"code","metadata":{"id":"uLhRX1VAvwen"},"source":["string = \"Similarity to word `crime`:\\n Black: \"\n","string += str(model.similarity(\"crime\", \"black\"))\n","string += \"\\n Latino: \" + str(model.similarity(\"crime\", \"latino\"))\n","string += \"\\n Asian: \" + str(model.similarity(\"crime\", \"asian\"))\n","string += \"\\n White: \" + str(model.similarity(\"crime\", \"white\"))\n","print(string)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMpeMnW7z1eX"},"source":["***What can we do?***\n","\n","\n","\n","1.   Train linear model for classification task\n","2.   Project to decision boundary\n","3.   goto 1\n","\n","\n","\n","<img src=\"https://lena-voita.github.io/resources/lectures/word_emb/papers/null_it_out-min.png\" alt=\"drawing\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"i7ykFjDy2JIp"},"source":["## Credentials and References\n","\n","1. YDS NLP Course. [Lecture about word embeddings.](https://lena-voita.github.io/nlp_course/word_embeddings.html)\n","2. [Paper about bias.](https://www.aclweb.org/anthology/2020.acl-main.647.pdf)"]},{"cell_type":"code","metadata":{"id":"xr6yVr3IxfYW"},"source":[""],"execution_count":null,"outputs":[]}]}