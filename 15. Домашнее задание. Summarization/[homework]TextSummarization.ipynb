{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kmb8UhIzOnfK"
   },
   "source": [
    "# Text Summarization. Homework\n",
    "\n",
    "Всем привет! Это домашка по суммаризации текста.\n",
    "\n",
    "На семинаре мы рассмотрели базовые модели для суммаризации текста. Попробуйте теперь улучшить два метода: TextRank и Extractive RNN. Задание достаточно большое и требует хорошую фантазию, тут можно эксперементировать во всю.\n",
    "\n",
    "Для сдачи заданий надо получить определенное качество по test-у:\n",
    "\n",
    "- 1 задание: 0.35 BLEU\n",
    "- 2 задание: 0.35 BLEU\n",
    "\n",
    "Если ваш подход пробивает это качество – задание считается пройденным. Плюсом будет описание того, почему вы решили использовать то или иное решение. \n",
    "\n",
    "Датасет: gazeta.ru\n",
    "\n",
    "**P.S.** Возможно, в датасете находятся пустые данные. Проверьте эту гипотезу, и если надо, сделайте предобратоку датасета.\n",
    "\n",
    "\n",
    "`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики МФТИ.`\n",
    "\n",
    "Загрузим датасет и необходимые библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqkLTkFRfXvA"
   },
   "source": [
    "##### !wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n",
    "!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n",
    "!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXS1sdYZCluU"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq razdel allennlp torch fasttext OpenNMT-py networkx pymorphy2 nltk rouge==0.3.1 summa\n",
    "!pip install -Uq transformers youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pZ2UGS2DGjH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(eval(line)) # Simple hack\n",
    "    records = pd.DataFrame(records)\n",
    "    if sort_by_date:\n",
    "        records = records.sort(\"date\")\n",
    "    if shuffle:\n",
    "        records = records.sample(frac=1)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNDp-BunEA91"
   },
   "outputs": [],
   "source": [
    "train_records = read_gazeta_records(\"gazeta_train.txt\")\n",
    "val_records = read_gazeta_records(\"gazeta_val.txt\")\n",
    "test_records = read_gazeta_records(\"gazeta_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsAcVSli3r3S"
   },
   "source": [
    "## 1 задание: TextRank (порог: 0.35 BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7jAQp-_Ds98"
   },
   "source": [
    "TextRank - unsupervised метод для составления кратких выжимок из текста. \n",
    "Описание метода:\n",
    "\n",
    "1. Сплитим текст по предложениям\n",
    "2. Считаем \"похожесть\" предложений между собой\n",
    "3. Строим граф предложений с взвешенными ребрами\n",
    "4. С помощью алгоритм PageRank получаем наиболее важные предложения, на основе которых делаем summary.\n",
    "\n",
    "Функция похожести можно сделать и из нейросетевых(или около) моделек: FastText, ELMO и BERT. Выберете один метод, загрузите предобученную модель и с ее помощью для каждого предложениия сделайте sentence embedding. С помощью косинусной меры определяйте похожесть предложений.\n",
    "\n",
    "Предобученные модели можно взять по [ссылке](http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "def calc_scores(references, predictions, metric=\"all\"):\n",
    "    print(\"Count:\", len(predictions))\n",
    "    print(\"Ref:\", references[-1])\n",
    "    print(\"Hyp:\", predictions[-1])\n",
    "\n",
    "    if metric in (\"bleu\", \"all\"):\n",
    "        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        print(\"ROUGE: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2GwyRrMPAzS"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "import razdel\n",
    "\n",
    "\n",
    "def your_super_words_similarity(words1, words2):\n",
    "    # Your code\n",
    "    pass\n",
    "\n",
    "\n",
    "def gen_text_rank_summary(text, calc_similarity=unique_words_similarity, summary_part=0.1, lower=True, morph=None):\n",
    "    '''\n",
    "    Составление summary с помощью TextRank\n",
    "    '''\n",
    "    # Разбиваем текст на предложения\n",
    "    sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    # Токенизируем предложения\n",
    "    sentences_words = [[token.text.lower() if lower else token.text for token in razdel.tokenize(sentence)] for sentence in sentences]\n",
    "\n",
    "    # При необходимости лемматизируем слова\n",
    "    if morph is not None:\n",
    "        sentences_words = [[morph.parse(word)[0].normal_form for word in words] for words in sentences_words]\n",
    "\n",
    "    # Для каждой пары предложений считаем близость\n",
    "    pairs = combinations(range(n_sentences), 2)\n",
    "    scores = [(i, j, calc_similarity(sentences_words[i], sentences_words[j])) for i, j in pairs]\n",
    "\n",
    "    # Строим граф с рёбрами, равными близости между предложениями\n",
    "    g = nx.Graph()\n",
    "    g.add_weighted_edges_from(scores)\n",
    "\n",
    "    # Считаем PageRank\n",
    "    pr = nx.pagerank(g)\n",
    "    result = [(i, pr[i], s) for i, s in enumerate(sentences) if i in pr]\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Выбираем топ предложений\n",
    "    n_summary_sentences = max(int(n_sentences * summary_part), 1)\n",
    "    result = result[:n_summary_sentences]\n",
    "\n",
    "    # Восстанавливаем оригинальный их порядок\n",
    "    result.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Восстанавливаем текст выжимки\n",
    "    predicted_summary = \" \".join([sentence for i, proba, sentence in result])\n",
    "    predicted_summary = predicted_summary.lower() if lower else predicted_summary\n",
    "    return predicted_summary\n",
    "\n",
    "def calc_text_rank_score(records, calc_similarity=unique_words_similarity, summary_part=0.1, lower=True, nrows=1000, morph=None):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for text, summary in records[['text', 'summary']].values[:nrows]:\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        predicted_summary = gen_text_rank_summary(text, calc_similarity, summary_part, lower, morph=morph)\n",
    "        text = text if not lower else text.lower()\n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_text_rank_score(test_records, calc_similarity=your_super_words_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdTrfxycB7cd"
   },
   "source": [
    "## 2 Задание: Extractive RNN (порог: 0.35 BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q7DeHDYFSjX"
   },
   "source": [
    "Второй метод, который вам предлагается улучшить – поиск предложений для summary с помощью RNN. В рассмотренной методе мы использовали LSTM для генерации sentence embedding. Попробуйте использовать другие архитектуры: CNN, Transformer; или добавьте предобученные модели, как и в первом задании.\n",
    "\n",
    "P.S. Тут предполагается, что придется изменять много кода в ячееках (например, поменять токенизацию). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dZamxigdEc-"
   },
   "source": [
    "### Модель\n",
    "\n",
    "Картинка для привлечения внимания:\n",
    "\n",
    "![img](https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_398421%2Fimages%2Farchitecture.png)\n",
    "\n",
    "Статья с оригинальным методом:\n",
    "https://arxiv.org/pdf/1611.04230.pdf\n",
    "\n",
    "Список вдохновения: \n",
    "- https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b Пример того, как можно применять CNN в текстовых задачах\n",
    "- https://arxiv.org/pdf/1808.08745.pdf Очень крутой метод генерации summary без Transformers\n",
    "- https://towardsdatascience.com/super-easy-way-to-get-sentence-embedding-using-fasttext-in-python-a70f34ac5b7c – простой метод генерации sentence embedding\n",
    "- https://towardsdatascience.com/fse-2b1ffa791cf9 – Необычный метод генерации sentence embedding\n",
    "- https://github.com/UKPLab/sentence-transformers – BERT предобученный для sentence embedding\n",
    "\n",
    "P.S. Выше написанные ссылки нужны только для разогрева вашей фантазии, можно воспользоваться ими, а можно придумать свой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOH4ZbLkg_sM"
   },
   "source": [
    "Комментарий к заданию:\n",
    "Если посмотреть на архитектуру ~~почти~~ SummaRuNNer, то в ней есть два главных элемента: первая часть, которая читает предложения и возвращает векторы на каждое предложение, и вторая, которая выбирает предложения для суммаризации. Вторую часть мы не трогаем, а первую меняем. На что меняем – как вы решите. Главное: она должна иметь хорошее качество и встроиться в текущую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sxsc0Orf8hGq"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
    "    '''\n",
    "    Жадное построение oracle summary\n",
    "    '''\n",
    "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
    "    # Делим текст на предложения\n",
    "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "    n_sentences = len(sentences)\n",
    "    oracle_summary_sentences = set()\n",
    "    score = -1.0\n",
    "    summaries = []\n",
    "    for _ in range(min(n_sentences, 2)):\n",
    "        for i in range(n_sentences):\n",
    "            if i in oracle_summary_sentences:\n",
    "                continue\n",
    "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
    "            # Добавляем какое-то предложения к уже существующему summary\n",
    "            current_summary_sentences.add(i)\n",
    "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
    "            # Считаем метрики\n",
    "            current_score = calc_score(current_summary, gold_summary)\n",
    "            summaries.append((current_score, current_summary_sentences))\n",
    "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
    "        # Иначе на этом заканчиваем\n",
    "        best_summary_score, best_summary_sentences = max(summaries)\n",
    "        if best_summary_score <= score:\n",
    "            break\n",
    "        oracle_summary_sentences = best_summary_sentences\n",
    "        score = best_summary_score\n",
    "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
    "    return oracle_summary, oracle_summary_sentences\n",
    "\n",
    "def calc_single_score(pred_summary, gold_summary, rouge):\n",
    "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7T_ak-KDB8rp"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def calc_oracle_score(records, nrows=1000, lower=True):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    rouge = Rouge()\n",
    "  \n",
    "    for text, summary in tqdm(records[['text', 'summary']].values[:nrows]):\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "        predicted_summary, _ = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge))\n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "calc_oracle_score(test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWgjewfWrbJZ"
   },
   "source": [
    "## (!)\n",
    "Если надо, поменяйте код загрузки токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qIRKm4TCHzN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=30000, lower=True):\n",
    "    temp_file_name = \"temp.txt\"\n",
    "    with open(temp_file_name, \"w\") as temp:\n",
    "        for text, summary in records[['text', 'summary']].values:\n",
    "            if lower:\n",
    "                summary = summary.lower()\n",
    "                text = text.lower()\n",
    "            if not text or not summary:\n",
    "                continue\n",
    "            temp.write(text + \"\\n\")\n",
    "            temp.write(summary + \"\\n\")\n",
    "    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n",
    "\n",
    "train_bpe(train_records, \"BPE_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAkZ2f5LhWwE"
   },
   "outputs": [],
   "source": [
    "bpe_processor = yttm.BPE('BPE_model.bin')\n",
    "bpe_processor.encode([\"октябрь богат на изменения\"], output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOkUL_YIGp-S"
   },
   "source": [
    "## (!)\n",
    "Если надо, поменяйте код словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhQYN1beiVEC"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, bpe_processor):\n",
    "        self.index2word = bpe_processor.vocab()\n",
    "        self.word2index = {w: i for i, w in enumerate(self.index2word)}\n",
    "        self.word2count = Counter()\n",
    "\n",
    "    def get_pad(self):\n",
    "        return self.word2index[\"<PAD>\"]\n",
    "\n",
    "    def get_sos(self):\n",
    "        return self.word2index[\"<SOS>\"]\n",
    "\n",
    "    def get_eos(self):\n",
    "        return self.word2index[\"<EOS>\"]\n",
    "\n",
    "    def get_unk(self):\n",
    "        return self.word2index[\"<UNK>\"]\n",
    "    \n",
    "    def has_word(self, word) -> bool:\n",
    "        return word in self.word2index\n",
    "\n",
    "    def get_index(self, word):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        return self.get_unk()\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.index2word)\n",
    "\n",
    "    def is_empty(self):\n",
    "        empty_size = 4\n",
    "        return self.size() <= empty_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.word2count = Counter()\n",
    "        self.index2word = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.word2index = {word: index for index, word in enumerate(self.index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qvZtNcOifAn"
   },
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(bpe_processor)\n",
    "vocabulary.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jdb-39jO-72q"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import razdel\n",
    "\n",
    "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
    "    rouge = Rouge()\n",
    "    sentences_ = []\n",
    "    oracle_sentences_ = []\n",
    "    oracle_summary_ = []\n",
    "    if nrows is not None:\n",
    "        records = records.iloc[:nrows].copy()\n",
    "    else:\n",
    "        records = records.copy()\n",
    "\n",
    "    for text, summary in tqdm(records[['text', 'summary']].values):\n",
    "        summary = summary.lower() if lower else summary\n",
    "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
    "                                                                         lower=lower, max_sentences=max_sentences)\n",
    "        sentences_ += [sentences]\n",
    "        oracle_sentences_ += [list(sentences_indicies)]\n",
    "        oracle_summary_ += [oracle_summary]\n",
    "    records['sentences'] = sentences_\n",
    "    records['oracle_sentences'] = oracle_sentences_\n",
    "    records['oracle_summary'] = oracle_summary_\n",
    "    return records\n",
    "\n",
    "ext_train_records = add_oracle_summary_to_records(train_records, nrows=30000)\n",
    "ext_val_records = add_oracle_summary_to_records(val_records, nrows=None)\n",
    "ext_test_records = add_oracle_summary_to_records(test_records, nrows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используй `pickle` для сохранения записей, чтобы потом не пересоздавать их потом. Если решаешь задание в колабе, можешь подключить свой гугл диск и сохранить данные в нём."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# with open(\"train_records.bin\", 'wb') as file:\n",
    "#     pickle.save(file, ext_train_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlXXc8qUHC5m"
   },
   "source": [
    "## (!)\n",
    "Если надо, поменяйте код генератора датасета и батчевалки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNyxstTChK3C"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import razdel\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class ExtDataset(data.Dataset):\n",
    "    def __init__(self, records, vocabulary, bpe_processor, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n",
    "        self.records = records\n",
    "        self.num_samples = records.shape[0]\n",
    "        self.bpe_processor = bpe_processor\n",
    "        self.lower = lower\n",
    "        self.rouge = Rouge()\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.records.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_record = self.records.iloc[idx]\n",
    "        inputs = list(map(lambda x: x[:self.max_sentence_length], self.bpe_processor.encode(cur_record['sentences'], output_type=yttm.OutputType.ID)))\n",
    "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
    "        return {'inputs': inputs, 'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvARjudojEDD"
   },
   "outputs": [],
   "source": [
    "# Это батчевалка\n",
    "def collate_fn(records):\n",
    "    max_length = max(len(sentence) for record in records for sentence in record['inputs'])\n",
    "    max_sentences = max(len(record['outputs']) for record in records)\n",
    "\n",
    "    new_inputs = torch.zeros((len(records), max_sentences, max_length))\n",
    "    new_outputs = torch.zeros((len(records), max_sentences))\n",
    "    for i, record in enumerate(records):\n",
    "        for j, sentence in enumerate(record['inputs']):\n",
    "            new_inputs[i, j, :len(sentence)] += np.array(sentence)\n",
    "        new_outputs[i, :len(record['outputs'])] += np.array(record['outputs'])\n",
    "    return {'features': new_inputs.type(torch.LongTensor), 'targets': new_outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWlf7XdheJUN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "class YourSentenceEncoder(nn.Module):\n",
    "    # Место для вашего Sentence Encoder-а. Разрешается использовать любые методы, которые вам нравятся.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SentenceTaggerRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 token_embedding_dim=256,\n",
    "                 sentence_encoder_hidden_size=256,\n",
    "                 hidden_size=256,\n",
    "                 bidirectional=True,\n",
    "                 sentence_encoder_n_layers=2,\n",
    "                 sentence_encoder_dropout=0.3,\n",
    "                 sentence_encoder_bidirectional=True,\n",
    "                 n_layers=1,\n",
    "                 dropout=0.3):\n",
    "        super(SentenceTaggerRNN, self).__init__()\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Your sentence encoder model\n",
    "        self.sentence_encoder = YourSentenceEncoder(...)\n",
    "        \n",
    "        self.rnn_layer = nn.LSTM(\n",
    "            sentence_encoder_hidden_size, \n",
    "            hidden_size, \n",
    "            n_layers, \n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
    "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        batch_size = inputs.size(0)\n",
    "        sentences_count = inputs.size(1)\n",
    "        tokens_count = inputs.size(2)\n",
    "        inputs = inputs.reshape(-1, tokens_count)\n",
    "        embedded_sentences = self.sentence_encoder(inputs)\n",
    "        embedded_sentences = embedded_sentences.reshape(batch_size, sentences_count, -1)\n",
    "        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
    "        content = self.content_linear_layer(outputs).squeeze(2)\n",
    "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2)\n",
    "        return content + salience\n",
    "\n",
    "model = SentenceTaggerRNN(vocabulary.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4q2Gb6ODHHB_"
   },
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVDW8raJeQxn"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "loaders = {\n",
    "    'train': data.DataLoader(\n",
    "        ExtDataset(\n",
    "            ext_train_records, \n",
    "            vocabulary, \n",
    "            bpe_processor=bpe_processor\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        collate_fn=collate_fn\n",
    "    ),\n",
    "    'valid': data.DataLoader(\n",
    "        ExtDataset(\n",
    "            ext_val_records, \n",
    "            vocabulary, \n",
    "            bpe_processor=bpe_processor\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        collate_fn=collate_fn\n",
    "    ),\n",
    "    'test': data.DataLoader(\n",
    "        ExtDataset(\n",
    "            ext_test_records, \n",
    "            vocabulary, \n",
    "            bpe_processor=bpe_processor\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        collate_fn=collate_fn\n",
    "    ),\n",
    "}\n",
    "\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Maybe adding scheduler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.to(device)\n",
    "    pbar_loader = trange(len(loaders[\"train\"]) + len(loaders[\"valid\"]), desc=f\"Train Loss: {0}, Valid Loss: {0}\")\n",
    "    for e in trange(num_epochs, desc=\"Epoch\"):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        train_it = 0\n",
    "        valid_it = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch in loaders[\"train\"]:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            \n",
    "            logits = model(features)\n",
    "            \n",
    "            loss = criterion(logits, targets)\n",
    "            train_loss += loss.item()\n",
    "            train_it += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Maybe adding scheduler?\n",
    "            \n",
    "            pbar_loader.update()\n",
    "            pbar_loader.set_description(\n",
    "                f\"Train Loss: {train_loss / train_it:.3}, Valid Loss: {0}\"\n",
    "            )\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in loaders[\"valid\"]:\n",
    "                features = batch[\"features\"].to(device)\n",
    "                targets = batch[\"targets\"].to(device)\n",
    "\n",
    "                logits = model(features)\n",
    "\n",
    "                loss = criterion(logits, targets)\n",
    "                valid_loss += loss.item()\n",
    "                valid_it += 1\n",
    "                \n",
    "                pbar_loader.update()\n",
    "                pbar_loader.set_description(\n",
    "                    f\"Train Loss: {train_loss / train_it:.3},\"\n",
    "                    f\" Valid Loss: {valid_loss / valid_it:.3}\"\n",
    "                )\n",
    "        print(\n",
    "            f\"Epoch {e}; Train Loss: {train_loss / train_it:.3},\"\n",
    "            f\" Valid Loss: {valid_loss / valid_it:.3}\"\n",
    "        )\n",
    "        pbar_loader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwqhK2dyKuGL"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "for num, item in tqdm(enumerate(loaders[\"test\"]), total=len(loaders[\"test\"])):\n",
    "    logits = model(item[\"features\"].to(device))[0]\n",
    "    \n",
    "    record = ext_test_records.iloc[num]\n",
    "    predicted_summary = []\n",
    "    for i, logit in enumerate(logits):\n",
    "        if logit > 0.0:\n",
    "            predicted_summary.append(record['sentences'][i])\n",
    "    \n",
    "    if not predicted_summary:\n",
    "        predicted_summary.append(record['sentences'][torch.max(logits, dim=0)[1].item()])\n",
    "    \n",
    "    predicted_summary = \" \".join(predicted_summary)\n",
    "    references.append(record['summary'].lower())\n",
    "    predictions.append(predicted_summary)\n",
    "\n",
    "calc_scores(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Kc0etEGfJ0p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]TextSummarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
